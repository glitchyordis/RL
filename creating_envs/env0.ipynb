{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapted from: https://medium.com/@paulswenson2/an-introduction-to-building-custom-reinforcement-learning-environment-using-openai-gym-d8a5e7cf07ea\n",
    "\n",
    "Creates a basic env a single player on 2D grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "# you can use check_env to check if your environment is working properly but \n",
    "# the tutorial which our code is based on seems to be outdated because check_env is not be satisfied with lots of things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTHING = 0\n",
    "PLAYER = 1\n",
    "WIN = 2\n",
    "LOSE = 3\n",
    "\n",
    "# action values\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicEnv(Env):\n",
    "    \"\"\"\n",
    "    A mxn grid\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, m, n) -> None:\n",
    "        self.m = m # horizontal/ rows\n",
    "        self.n = n # vertical/ columns\n",
    "        self.total_states = m * n\n",
    "\n",
    "        # init state\n",
    "        self.reset()\n",
    "\n",
    "        self.observation_space = spaces.Box(0, 3, [self.total_states,], dtype=np.int16)\n",
    "        self.action_space = spaces.Discrete(4) # shortcut for defining the actions 0-3\n",
    "\n",
    "    def print_grid_idx(self):\n",
    "        \"\"\"\n",
    "        helper func to print the grid indices\n",
    "        \"\"\"\n",
    "        print(np.array(np.arange(self.total_states, dtype=np.int32)).reshape(self.m, self.n))\n",
    "\n",
    "    def print_state_as_grid(self):\n",
    "        \"\"\"\n",
    "        helper func to print the grid\n",
    "        \"\"\"\n",
    "        print(self.state.reshape(self.m, self.n))\n",
    "\n",
    "    def step(self, action):\n",
    "        info = {}\n",
    "\n",
    "        done = False\n",
    "        reward = -0.01\n",
    "        previous_position = self.player_pos\n",
    "\n",
    "        if action == UP:\n",
    "            if (self.player_pos - self.m) >= 0:\n",
    "                self.player_pos -= self.m\n",
    "        elif action == DOWN:\n",
    "            if (self.player_pos + self.m) < self.total_states:\n",
    "                self.player_pos += self.m\n",
    "        elif action == LEFT:\n",
    "            # check column index; == 0 means leftmost column\n",
    "            if (self.player_pos % self.n) != 0:\n",
    "                self.player_pos -= 1\n",
    "        elif action == RIGHT:\n",
    "            if (self.player_pos % self.n) != self.n - 1:\n",
    "                self.player_pos += 1\n",
    "        else:\n",
    "            # check for invalid actions\n",
    "            raise Exception(\"invalid action\")\n",
    "        \n",
    "        if self.state[self.player_pos] == WIN:\n",
    "            reward = 1\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print('YOU WIN!!!!')\n",
    "        elif self.state[self.player_pos] == LOSE:\n",
    "            reward = -1.0\n",
    "            self.cumulative_reward += reward\n",
    "            done = True\n",
    "            clear_screen()\n",
    "            print(f'Cumulative Reward: {self.cumulative_reward}')\n",
    "            print('YOU LOSE')\n",
    "\n",
    "        if not done:\n",
    "            self.state[previous_position] = NOTHING\n",
    "            self.state[self.player_pos] = PLAYER\n",
    "        \n",
    "        self.cumulative_reward += reward\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pretty_print(self.state, self.cumulative_reward, self.m, self.n)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_reward = 0\n",
    "        self.state = [NOTHING] * self.total_states\n",
    "        self.player_pos = random.randrange(0, self.total_states)\n",
    "        self.win_pos = random.randrange(0, self.total_states)\n",
    "        self.lose_pos = random.randrange(0, self.total_states) \n",
    "\n",
    "        while self.win_pos == self.player_pos:\n",
    "            self.win_pos = random.randrange(0, self.total_states)\n",
    "        while self.lose_pos == self.player_pos or self.lose_pos == self.win_pos:\n",
    "            self.lose_pos = random.randrange(0, self.total_states)\n",
    "\n",
    "        # assign player and win/lose positions\n",
    "        self.state[self.player_pos] = PLAYER\n",
    "        self.state[self.win_pos] = WIN\n",
    "        self.state[self.lose_pos] = LOSE\n",
    "\n",
    "        # convert to array (gym requirement)\n",
    "        self.state = np.array(self.state, dtype=np.int32)\n",
    "    \n",
    "def pretty_print(state_array, cumulative_reward, m, n):\n",
    "    clear_screen()\n",
    "    print(f'Cumulative Reward: {cumulative_reward}')\n",
    "    print()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            print('{:4}'.format(state_array[i*6 + j]), end = \"\")\n",
    "        print()\n",
    "\n",
    "def clear_screen():\n",
    "    clear_output()\n",
    "    os.system(\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual entry of actions\n",
    "env = BasicEnv(6, 6)\n",
    "env.render()\n",
    "time.sleep(0.001)\n",
    "action = int(input(\"Enter action:\"))\n",
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.001)\n",
    "    action = int(input(\"Enter action:\"))\n",
    "    state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random actions\n",
    "env = BasicEnv(6, 6)\n",
    "env.render()\n",
    "time.sleep(0.01)\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, info = env.step(action)\n",
    "\n",
    "steps = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "    if steps % 15 == 0:\n",
    "        print(f'{steps} Steps reached. Ending.')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
