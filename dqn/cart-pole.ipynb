{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4f2d8f",
   "metadata": {},
   "source": [
    "# Deep Q-learning Tutorial\n",
    "\n",
    "source: https://github.com/yuumii-san/RL_tutorial_notebook/tree/main\n",
    "\n",
    "In this series of notebooks, I have introduced various policy iteration algorithms to solve Markov Decision Processes (MDPs). While these methods, which utilize tabular estimates of state/action values, work well in simpler settings, they face challenges in complex environments due to the great number of states and actions, making it impractical to learn with tabular estimates.\n",
    "\n",
    "One solution to this problem is to employ a sophisticated function to efficiently estimate values. In this notebook, I will introduce one such approach known as Deep Q-learning (DQN). DQN utilizes a neural network to efficiently estimate state values. To address challenges when applying a neural network to estimate the value function, DQN incorporates techniques such as experience replay and a target Q-network.\n",
    "\n",
    "We will learn these crucial conceptual elements with the implementation. So, let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84dc2f6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d371fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable # Variable is depreciated in newer versions of PyTorch\n",
    "from typing import List, Tuple\n",
    "import torchinfo\n",
    "\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f06203",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3\n",
    "print(x)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3646f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d981828",
   "metadata": {},
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55f390",
   "metadata": {},
   "source": [
    "In this notebook, we utilize the CartPole-v0 environment in Gymnasium.\n",
    "\n",
    "[Learn more about the CartPole-v0 environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "The goal of this environment is to balance a pole by applying forces in the left and right directions on the cart. It has a discrete action space:\n",
    "- 0: Push cart to the left\n",
    "- 1: Push cart to the right\n",
    "\n",
    "Upon taking an action, either left or right, an agent observes a 4-dimensional state consisting of:\n",
    "- Cart Position\n",
    "- Cart Velocity\n",
    "- Pole Angle\n",
    "- Pole Angular Velocity\n",
    "\n",
    "A reward of +1 is granted to the agent at each step while the pole is kept upright. The maximum reward an agent can earn in a single episode is 200.\n",
    "\n",
    "The episode ends under the following conditions:\n",
    "- Termination: Pole Angle is greater than ±12°\n",
    "- Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "- Truncation: Episode length exceeds 200 steps\n",
    "\n",
    "In the code below, I provide an example of the agent randomly exploring this environment over 100 time steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391e4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# # Run the environment for 100 steps\n",
    "# for i in range(100):\n",
    "#     print(f\"Step {i + 1}\")\n",
    "#     # Display the current state of the environment\n",
    "#     plt.imshow(env.render())\n",
    "#     display.display(plt.gcf())\n",
    "#     display.clear_output(wait=True)\n",
    "    \n",
    "#     # Choose a random action from the action space\n",
    "#     action = env.action_space.sample()\n",
    "    \n",
    "#     # Take the chosen action and observe the next state, reward, and termination status\n",
    "#     state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "#     # If the episode is terminated or truncated, reset the environment\n",
    "#     if terminated or truncated:\n",
    "#         state, info = env.reset()\n",
    "\n",
    "# # Close the environment after exploration\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce5c88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_space = Discrete(2)\n",
      "env.observation_space = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{env.action_space = }\")\n",
    "print(f\"{env.observation_space = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ceeb8",
   "metadata": {},
   "source": [
    "### Define Q-Network\n",
    "\n",
    "Let's begin by preparing a model to estimate a value function. Here, we assume that the environment dynamics is unknow and the agent needs to learn the optimal policy by interacting with the environment. State space of CartPole-v0 is continuous, and the number of possible states is infinitely large. Therefore, applying a tabular estimate strategy, as done in Monte Carlo Methods or TD-learning, is not feasible. This is why we use a model to estimate the value based on the observed state.\n",
    "\n",
    "For Deep Q-Network (DQN), we employ a deep learning model as a value function estimator. The estimated value function is referred to as the Q-value, and the neural network used for this purpose is known as the Q-network. This network takes a state ($\\mathbb{R}^{Nx4}$) as an input and outputs Q values of the actions available to the agent ($\\mathbb{R}^{Nx2}$). To keep it simple, we will utilize a fully connected neural network with ReLU activation. We use the Adam optimizer for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ab7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(FullyConnectedModel, self).__init__()\n",
    "\n",
    "        # Define layers with ReLU activation\n",
    "        self.linear1 = nn.Linear(input_size, 16)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(16, 16)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(16, 16)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        # Output layer without activation function\n",
    "        self.output_layer = nn.Linear(16, output_size)\n",
    "\n",
    "        # Initialization using Xavier uniform (a popular technique for initializing weights in NNs)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        nn.init.xavier_uniform_(self.linear3.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Forward pass through the layers\n",
    "        x = self.activation1(self.linear1(inputs))\n",
    "        x = self.activation2(self.linear2(x))\n",
    "        x = self.activation3(self.linear3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "class QNetwork:\n",
    "    def __init__(self, env, lr, input_size=4, logdir=None):\n",
    "        # Define Q-network with specified architecture\n",
    "        self.input_size = input_size\n",
    "        self.net = FullyConnectedModel(4, 2)\n",
    "        self.env = env\n",
    "        self.lr = lr \n",
    "        self.logdir = logdir\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        # Load pre-trained model from a file\n",
    "        return self.net.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    def load_model_weights(self, weight_file):\n",
    "        # Load pre-trained model weights from a file\n",
    "        return self.net.load_state_dict(torch.load(weight_file))\n",
    "    \n",
    "    def print_state_dict_and_info(self):\n",
    "        summary = torchinfo.summary(self.net, input_size=(1, self.input_size),\n",
    "                          col_width=16, col_names=[\"output_size\",\"kernel_size\", \"num_params\"],  verbose=2)\n",
    "        \n",
    "        print(\"\\nModel's state_dict:\")\n",
    "        for param_tensor in self.net.state_dict():\n",
    "            print(param_tensor, \"\\t\", self.net.state_dict()[param_tensor].size())\n",
    "\n",
    "        print(\"\\nOptimizer's state_dict:\")\n",
    "        for var_name in self.optimizer.state_dict():\n",
    "            print(var_name, \"\\t\", self.optimizer.state_dict()[var_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848de8bc",
   "metadata": {},
   "source": [
    "Note: The Xavier initialization (or Glorot initialization) is a popular technique for initializing weights in a neural network. For more information, you can check [this article](https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8800e5",
   "metadata": {},
   "source": [
    "### Define Replay Memory\n",
    "\n",
    "Next, we introduce one of the key components of DQN, Experience Replay.\n",
    "\n",
    "While deep learning models or other complex ML models for estimating a value function may seem appealing, they did not enjoy success in RL for a long time until Experience Replay was introduced. This is due to a correlation within the training dataset. In supervised learning, a crucial assumption is that all training data is independent and identically distributed (i.i.d). However, consecutive samples collected during the agent's interactions with the environment in RL are highly correlated, breaking this assumption and causing issues in fitting a machine learning model.\n",
    "\n",
    "Experience Replay mitigates this temporal correlation by randomly sampling experiences from a replay memory. This approach helps decorrelate training samples, promoting better convergence. By storing and reusing past experiences, Experience Replay enables the agent to learn from a diverse set of transitions, making more efficient use of the collected data. This diversity aids in better exploration and can lead to more stable and faster convergence of the Q-network.\n",
    "\n",
    "Specifically, we first define a replay memory with a specified memory size. On each iteration, we store a new observation in this replay memory and randomly sample data from this memory to train a model. This random sampling from a replay memory breaks the correlational structure in the training data.\n",
    "\n",
    "Because of Replay Memory, DQN is classified as an off-policy method. On-Policy methods use the same policy for both collecting samples and updating the value function. \n",
    "\n",
    "In contrast, Off-Policy methods use two distinct policies for collecting samples and updating a value function, respectively. For DQN, experience samples are collected with an epsilon-greedy policy by using the Q-network estimate at the time. The accumulated experience in the memory buffer is then used to update the Q-network, consequently updating the policy by taking greedy action based on Q-network estimate. The data accumulated in the memory buffer does not follow the same policy as the current policy because we keep updating the Q-network. Thus, the data used to update our target policy is collected with different policies (i.e., Off-policy).\n",
    "\n",
    "Let's set up the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05af9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, env, memory_size=int(5e4), burn_in=int(1e4)):\n",
    "        # Initializes the replay memory, which stores transitions recorded from the agent taking actions in the environment.\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.memory = collections.deque([], maxlen=memory_size)\n",
    "        self.env = env\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        # Returns a batch of randomly sampled transitions to be used for training the model.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def append(self, transition):\n",
    "        # Appends a transition to the replay memory.\n",
    "        self.memory.append(transition)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052bb56",
   "metadata": {},
   "source": [
    "### Q-Value Function Update\n",
    "\n",
    "As we discussed in the above section, to address the correlation problem in training data, we leverage a replay buffer. Samples extracted from the memory buffer are used to train our model. Let's learn the detailed process of updating a model.\n",
    "\n",
    "Firstly, we utilize the Temporal Difference (TD) error to update the value of a state and action pair, following the principles of Q-learning, which is why this approach is termed Deep \"Q-learning\". Q-learning is an Off-policy TD learning algorithm.\n",
    "\n",
    "The TD error is given by the formula:\n",
    "\n",
    "$$ (R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a)) - Q(S_t, A_t) $$\n",
    "\n",
    "A crucial difference between an original Q-learning and DQN is that the current estimate of the Q-value $Q(S_t, A_t)$ and $Q(S_{t+1}, a)$ is obtained from a deep learning model. The model updates the prediction to minimize this TD error, meaning it uses the mean squared error between the estimated Q-value and the sum of the immediate reward and the value of the best next state-aciton pair.\n",
    "\n",
    "However, there is a challenge. While our aim is to update the Q-value estimate by minimizing the TD error, the computation of the next state value $Q(S_{t+1}, a)$ within TD error equation also relies on the network, which we update at each iteration. As our value estimate for the next state fluctuates every iteration due to the network update, the error fluctuates too, leading to instability in the Q-network fitting.\n",
    "\n",
    "To overcome this issue, DQN introduces another neural network called the target network. This network is updated more slowly compared to the policy Q-network, which we use to define the optimal policy based on the value estimation. Unlike the policy network, the target network is not updated every iteration. Instead, it is updated once in a while to stabilize the learning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef98c3",
   "metadata": {},
   "source": [
    "### Define DQN Agent\n",
    "\n",
    "Now that we have covered all the essential components (Deep Learning Model, Replay Memory, and Target Network), we can proceed with the implementation to train a DQN agent. The training of DQN in the below implementation roughly follows these steps:\n",
    "\n",
    "1. Initialize Q-network and Target Network.\n",
    "2. Initialize Replay Memory using burn-in with a random action policy.\n",
    "3. Initialize the environment.\n",
    "4. For each episode from 1 to M:\n",
    "    1. Choose an action based on the current Q-value estimate using an epsilon-greedy policy.\n",
    "    2. Take a step and observe the next state and reward.\n",
    "    3. Store the new experience into the replay memory.\n",
    "    4. Sample a minibatch with size N from the replay memory.\n",
    "    5. Train the network with the batched dataset.\n",
    "    6. Go back to step 1.\n",
    "5. Every 10 episodes, perform 20 test episodes to evaluate the performance of the current agent.\n",
    "\n",
    "Let's explore the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee0c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, environment_name, lr=5e-4, render=False):\n",
    "        # Initialize the DQN Agent.\n",
    "        self.env = gym.make(environment_name)\n",
    "        self.lr = lr # alpha\n",
    "        self.policy_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net = QNetwork(self.env, self.lr)\n",
    "        self.target_net.net.load_state_dict(self.policy_net.net.state_dict())  # Copy the weight of the policy network\n",
    "        self.rm = ReplayMemory(self.env)\n",
    "        self.burn_in_memory()\n",
    "        # self.batch_size = 32\n",
    "        self.batch_size = 256\n",
    "        self.gamma = 0.99\n",
    "        self.c = 0\n",
    "\n",
    "    def burn_in_memory(self):\n",
    "        # Initialize replay memory with a burn-in number of episodes/transitions.\n",
    "        cnt = 0\n",
    "        terminated, truncated  = False, False\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Iterate until we store \"burn_in\" buffer\n",
    "        while cnt < self.rm.burn_in:\n",
    "            # Reset environment if terminated or truncated\n",
    "            if terminated or truncated:\n",
    "                state, _ = self.env.reset()\n",
    "                state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            # Randomly select an action (left or right) and take a step\n",
    "            action = torch.tensor(random.sample([0, 1], 1)[0]).reshape(1, 1)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "                \n",
    "            # Store new experience into memory\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "            state = next_state\n",
    "            cnt += 1\n",
    "\n",
    "    def epsilon_greedy_policy(self, q_values, epsilon=0.05):\n",
    "        # Implement an epsilon-greedy policy. \n",
    "        p = random.random()\n",
    "        if p > epsilon:\n",
    "            with torch.no_grad():\n",
    "                return self.greedy_policy(q_values)\n",
    "        else:\n",
    "            return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "\n",
    "    def greedy_policy(self, q_values):\n",
    "        # Implement a greedy policy for test time.\n",
    "        return torch.argmax(q_values)\n",
    "        \n",
    "    def train(self):\n",
    "        # Train the Q-network using Deep Q-learning.\n",
    "        state, _ = self.env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        terminated,truncated = False, False\n",
    "\n",
    "        # Loop until reaching the termination state\n",
    "        while not (terminated or truncated):\n",
    "            with torch.no_grad():\n",
    "                \"\"\"\n",
    "                example: tensor([[ 0.0149, -0.0221]])\n",
    "                \"\"\"\n",
    "                q_values = self.policy_net.net(state)\n",
    "\n",
    "            # Decide the next action with epsilon greedy strategy\n",
    "            action = self.epsilon_greedy_policy(q_values).reshape(1, 1)\n",
    "            \n",
    "            # Take action and observe reward and next state\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            reward = torch.tensor([reward])\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Store the new experience\n",
    "            transition = Transition(state, action, next_state, reward)\n",
    "            self.rm.memory.append(transition)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Sample minibatch with size N from memory\n",
    "            transitions = self.rm.sample_batch(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Get current and next state values\n",
    "            \"\"\"\n",
    "            example usage of gather\n",
    "            tensor = torch.tensor([[10, 20, 30], \n",
    "                       [40, 50, 60]])\n",
    "\n",
    "            # Indices to gather\n",
    "            indices = torch.tensor([[0], [2]])\n",
    "\n",
    "            # Gather values\n",
    "            result = tensor.gather(1, indices)\n",
    "            >>tensor([[10],\n",
    "                    [60]])\n",
    "            \"\"\"\n",
    "            state_action_values = self.policy_net.net(state_batch).gather(1, action_batch) # extract values corresponding to the actions Q(S_t, A_t)\n",
    "            next_state_values = torch.zeros(self.batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # no next_state_value update if an episode is terminated (next_satate = None)\n",
    "                # only update the non-termination state values (Ref: https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/)\n",
    "                next_state_values[non_final_mask] = self.target_net.net(non_final_next_states).max(1)[0] # extract max value\n",
    "                \n",
    "            # Update the model\n",
    "            expected_state_action_values = reward_batch + (next_state_values * self.gamma)\n",
    "            \n",
    "            criterion = torch.nn.MSELoss()\n",
    "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            \n",
    "            self.policy_net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policy_net.optimizer.step()\n",
    "\n",
    "            # Update the target Q-network in each 50 steps\n",
    "            self.c += 1\n",
    "            if self.c % 50 == 0:\n",
    "                self.target_net.net.load_state_dict(self.policy_net.net.state_dict())\n",
    "\n",
    "    def test(self, model_file=None):\n",
    "        # Evaluates the performance of the agent over 20 episodes.\n",
    "\n",
    "        max_t = 1000\n",
    "        state, _ = self.env.reset()\n",
    "        rewards = []\n",
    "\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net.net(state)\n",
    "            action = self.greedy_policy(q_values)\n",
    "            state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        return np.sum(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff675006",
   "metadata": {},
   "source": [
    "### Train and Test DQN Agent\n",
    "\n",
    "Let's train a DQN agent to evaluate its performance on the CartPole-v0 environment. In this environment, the maximum achievable reward for an agent in each episode is 200. We will train the model over 200 episodes, evaluating its performance every 10 episodes with an additional 20 test episodes. This training process will be repeated five times, allowing us to assess the average performance of the agent across episodes.\n",
    "\n",
    "Please note that this section may take a few minutes to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d0bae4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felikong\\AppData\\Local\\miniconda3\\envs\\py310_2env\\lib\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The test reward for episode 0 is 12.75 with a standard deviation of 1.9716744153130354.\n",
      "Episode: 10\n",
      "The test reward for episode 10 is 9.6 with a standard deviation of 0.58309518948453.\n",
      "Episode: 20\n",
      "The test reward for episode 20 is 9.1 with a standard deviation of 0.7000000000000001.\n",
      "Episode: 30\n",
      "The test reward for episode 30 is 9.75 with a standard deviation of 0.698212002188447.\n",
      "Episode: 40\n",
      "The test reward for episode 40 is 16.35 with a standard deviation of 2.4753787588973126.\n",
      "Episode: 50\n",
      "The test reward for episode 50 is 9.45 with a standard deviation of 0.6689544080129824.\n",
      "Episode: 60\n",
      "The test reward for episode 60 is 9.25 with a standard deviation of 0.6224949798994366.\n",
      "Episode: 70\n",
      "The test reward for episode 70 is 36.8 with a standard deviation of 11.183022847155414.\n",
      "Episode: 80\n",
      "The test reward for episode 80 is 78.65 with a standard deviation of 28.922785135598545.\n",
      "Episode: 90\n",
      "The test reward for episode 90 is 199.0 with a standard deviation of 2.701851217221259.\n",
      "Episode: 100\n",
      "The test reward for episode 100 is 194.95 with a standard deviation of 12.188006399735766.\n",
      "Episode: 110\n",
      "The test reward for episode 110 is 199.15 with a standard deviation of 3.705064102009572.\n",
      "Episode: 120\n",
      "The test reward for episode 120 is 197.6 with a standard deviation of 7.227724399837061.\n",
      "Episode: 130\n",
      "The test reward for episode 130 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 140\n",
      "The test reward for episode 140 is 199.55 with a standard deviation of 1.9615045245933027.\n",
      "Episode: 150\n",
      "The test reward for episode 150 is 194.2 with a standard deviation of 8.806815542521598.\n",
      "Episode: 160\n",
      "The test reward for episode 160 is 171.25 with a standard deviation of 7.482479535555042.\n",
      "Episode: 170\n",
      "The test reward for episode 170 is 169.75 with a standard deviation of 6.409953197957065.\n",
      "Episode: 180\n",
      "The test reward for episode 180 is 157.25 with a standard deviation of 7.251723932969318.\n",
      "Episode: 190\n",
      "The test reward for episode 190 is 198.3 with a standard deviation of 4.428317965096906.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:11<08:46, 131.53s/it]c:\\Users\\felikong\\AppData\\Local\\miniconda3\\envs\\py310_2env\\lib\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The test reward for episode 0 is 9.4 with a standard deviation of 0.7348469228349535.\n",
      "Episode: 10\n",
      "The test reward for episode 10 is 9.55 with a standard deviation of 0.5894913061275798.\n",
      "Episode: 20\n",
      "The test reward for episode 20 is 9.15 with a standard deviation of 0.7262919523166975.\n",
      "Episode: 30\n",
      "The test reward for episode 30 is 9.3 with a standard deviation of 0.7810249675906653.\n",
      "Episode: 40\n",
      "The test reward for episode 40 is 9.5 with a standard deviation of 0.5.\n",
      "Episode: 50\n",
      "The test reward for episode 50 is 9.5 with a standard deviation of 0.5916079783099616.\n",
      "Episode: 60\n",
      "The test reward for episode 60 is 9.35 with a standard deviation of 0.7262919523166976.\n",
      "Episode: 70\n",
      "The test reward for episode 70 is 114.7 with a standard deviation of 26.745279957405568.\n",
      "Episode: 80\n",
      "The test reward for episode 80 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 90\n",
      "The test reward for episode 90 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 100\n",
      "The test reward for episode 100 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 110\n",
      "The test reward for episode 110 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 120\n",
      "The test reward for episode 120 is 198.75 with a standard deviation of 5.448623679425842.\n",
      "Episode: 130\n",
      "The test reward for episode 130 is 103.65 with a standard deviation of 18.322868225253384.\n",
      "Episode: 140\n",
      "The test reward for episode 140 is 191.0 with a standard deviation of 15.215124054702938.\n",
      "Episode: 150\n",
      "The test reward for episode 150 is 94.45 with a standard deviation of 39.63644156581164.\n",
      "Episode: 160\n",
      "The test reward for episode 160 is 92.9 with a standard deviation of 36.99445904456504.\n",
      "Episode: 170\n",
      "The test reward for episode 170 is 150.3 with a standard deviation of 5.866003750424986.\n",
      "Episode: 180\n",
      "The test reward for episode 180 is 151.2 with a standard deviation of 9.201086892318756.\n",
      "Episode: 190\n",
      "The test reward for episode 190 is 133.85 with a standard deviation of 4.639773701378117.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:09<06:09, 123.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The test reward for episode 0 is 9.7 with a standard deviation of 0.9539392014169455.\n",
      "Episode: 10\n",
      "The test reward for episode 10 is 9.1 with a standard deviation of 0.7000000000000001.\n",
      "Episode: 20\n",
      "The test reward for episode 20 is 9.5 with a standard deviation of 1.02469507659596.\n",
      "Episode: 30\n",
      "The test reward for episode 30 is 9.3 with a standard deviation of 0.714142842854285.\n",
      "Episode: 40\n",
      "The test reward for episode 40 is 12.9 with a standard deviation of 4.036087214122113.\n",
      "Episode: 50\n",
      "The test reward for episode 50 is 26.9 with a standard deviation of 4.515528761950255.\n",
      "Episode: 60\n",
      "The test reward for episode 60 is 98.25 with a standard deviation of 12.632794623518583.\n",
      "Episode: 70\n",
      "The test reward for episode 70 is 199.2 with a standard deviation of 2.4207436873820405.\n",
      "Episode: 80\n",
      "The test reward for episode 80 is 197.5 with a standard deviation of 4.9749371855331.\n",
      "Episode: 90\n",
      "The test reward for episode 90 is 199.1 with a standard deviation of 2.981610303175114.\n",
      "Episode: 100\n",
      "The test reward for episode 100 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 110\n",
      "The test reward for episode 110 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 120\n",
      "The test reward for episode 120 is 182.0 with a standard deviation of 10.559356040971437.\n",
      "Episode: 130\n",
      "The test reward for episode 130 is 157.65 with a standard deviation of 34.68036187815808.\n",
      "Episode: 140\n",
      "The test reward for episode 140 is 188.1 with a standard deviation of 16.849035580709064.\n",
      "Episode: 150\n",
      "The test reward for episode 150 is 200.0 with a standard deviation of 0.0.\n",
      "Episode: 160\n",
      "The test reward for episode 160 is 111.45 with a standard deviation of 10.763247651150651.\n",
      "Episode: 170\n",
      "The test reward for episode 170 is 160.15 with a standard deviation of 45.668670880593844.\n",
      "Episode: 180\n",
      "The test reward for episode 180 is 55.05 with a standard deviation of 4.443815927780988.\n",
      "Episode: 190\n",
      "The test reward for episode 190 is 60.95 with a standard deviation of 4.318275118609281.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [06:13<04:08, 124.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The test reward for episode 0 is 9.1 with a standard deviation of 0.8888194417315589.\n",
      "Episode: 10\n",
      "The test reward for episode 10 is 11.1 with a standard deviation of 0.99498743710662.\n",
      "Episode: 20\n",
      "The test reward for episode 20 is 9.45 with a standard deviation of 0.6689544080129824.\n",
      "Episode: 30\n",
      "The test reward for episode 30 is 9.5 with a standard deviation of 0.6708203932499369.\n",
      "Episode: 40\n",
      "The test reward for episode 40 is 9.45 with a standard deviation of 0.739932429347437.\n",
      "Episode: 50\n",
      "The test reward for episode 50 is 23.15 with a standard deviation of 6.381810088054955.\n",
      "Episode: 60\n",
      "The test reward for episode 60 is 116.1 with a standard deviation of 33.64060047026509.\n",
      "Episode: 70\n",
      "The test reward for episode 70 is 187.9 with a standard deviation of 17.256592942988487.\n",
      "Episode: 80\n",
      "The test reward for episode 80 is 191.4 with a standard deviation of 16.53602128687551.\n",
      "Episode: 90\n",
      "The test reward for episode 90 is 197.1 with a standard deviation of 7.7517739905133975.\n",
      "Episode: 100\n",
      "The test reward for episode 100 is 190.1 with a standard deviation of 10.59669759878048.\n",
      "Episode: 110\n",
      "The test reward for episode 110 is 197.45 with a standard deviation of 5.084043666216882.\n",
      "Episode: 120\n",
      "The test reward for episode 120 is 192.0 with a standard deviation of 9.57078889120432.\n",
      "Episode: 130\n",
      "The test reward for episode 130 is 173.3 with a standard deviation of 25.35566997734432.\n",
      "Episode: 140\n",
      "The test reward for episode 140 is 133.9 with a standard deviation of 21.812611031236035.\n",
      "Episode: 150\n",
      "The test reward for episode 150 is 150.0 with a standard deviation of 33.324165405903265.\n",
      "Episode: 160\n",
      "The test reward for episode 160 is 157.55 with a standard deviation of 24.113222513799354.\n",
      "Episode: 170\n",
      "The test reward for episode 170 is 138.15 with a standard deviation of 4.92214384186403.\n",
      "Episode: 180\n",
      "The test reward for episode 180 is 179.15 with a standard deviation of 4.102133591193733.\n",
      "Episode: 190\n",
      "The test reward for episode 190 is 198.45 with a standard deviation of 2.8892040426387333.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [08:46<02:15, 135.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "The test reward for episode 0 is 9.4 with a standard deviation of 0.8602325267042626.\n",
      "Episode: 10\n",
      "The test reward for episode 10 is 9.05 with a standard deviation of 0.7399324293474371.\n",
      "Episode: 20\n",
      "The test reward for episode 20 is 9.55 with a standard deviation of 0.739932429347437.\n",
      "Episode: 30\n",
      "The test reward for episode 30 is 8.85 with a standard deviation of 0.6538348415311012.\n",
      "Episode: 40\n",
      "The test reward for episode 40 is 9.5 with a standard deviation of 0.8660254037844386.\n",
      "Episode: 50\n",
      "The test reward for episode 50 is 26.05 with a standard deviation of 6.264782518172518.\n",
      "Episode: 60\n",
      "The test reward for episode 60 is 128.35 with a standard deviation of 47.41336836800355.\n",
      "Episode: 70\n",
      "The test reward for episode 70 is 179.25 with a standard deviation of 20.777090749188154.\n",
      "Episode: 80\n",
      "The test reward for episode 80 is 162.4 with a standard deviation of 15.311433636338565.\n",
      "Episode: 90\n",
      "The test reward for episode 90 is 164.55 with a standard deviation of 23.32482583000353.\n",
      "Episode: 100\n",
      "The test reward for episode 100 is 187.8 with a standard deviation of 15.347964034359737.\n",
      "Episode: 110\n",
      "The test reward for episode 110 is 174.75 with a standard deviation of 14.76101283787803.\n",
      "Episode: 120\n",
      "The test reward for episode 120 is 162.05 with a standard deviation of 18.6372610648668.\n",
      "Episode: 130\n",
      "The test reward for episode 130 is 138.15 with a standard deviation of 11.145739096174825.\n",
      "Episode: 140\n",
      "The test reward for episode 140 is 121.25 with a standard deviation of 15.394398331860847.\n",
      "Episode: 150\n",
      "The test reward for episode 150 is 108.85 with a standard deviation of 18.34741126153769.\n",
      "Episode: 160\n",
      "The test reward for episode 160 is 124.6 with a standard deviation of 37.38234877585944.\n",
      "Episode: 170\n",
      "The test reward for episode 170 is 158.55 with a standard deviation of 31.48408328028625.\n",
      "Episode: 180\n",
      "The test reward for episode 180 is 128.0 with a standard deviation of 6.920982589199311.\n",
      "Episode: 190\n",
      "The test reward for episode 190 is 136.95 with a standard deviation of 12.54780857361157.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:10<00:00, 134.18s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Avg. Return')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAG2CAYAAAB4e1KRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4+klEQVR4nO3deXwTZf4H8M9Mrt4tLfSCUsp9I5cVRQRBAV1QQVdYVFzxwAUv1FX2t57rLq54H6vrrorrgccquODJjWgBOSqgUCiWlqMHUHq3aZJ5fn+kjRR6pMkkmUk+79dmpZPJzJMmmX7zfb7P80hCCAEiIiKiECMHugFEREREgcAgiIiIiEISgyAiIiIKSQyCiIiIKCQxCCIiIqKQxCCIiIiIQhKDICIiIgpJxkA3QMsURcGxY8cQHR0NSZIC3RwiIiJygxAClZWVSE1NhSy3nO9hENSKY8eOIS0tLdDNICIiIg8cPnwYXbp0afF+BkGtiI6OBuD8JcbExAS4NUREROSOiooKpKWluf6Ot4RBUCsau8BiYmIYBBEREelMW6UsLIwmIiKikMQgiIiIiEISgyAiIiIKSQyCiIiIKCQxCCIiIqKQxCCIiIiIQhKDICIiIgpJDIKIiIgoJDEIIiIiopDEIIiIiIhCkiaDoEWLFmHkyJGIjo5GYmIirrzySuTk5DTZp66uDvPmzUNCQgKioqIwffp0FBcXN9mnoKAAl19+OSIiIpCYmIj7778fdrvdn0+FiIiINEqTQdCGDRswb948bN68GatWrYLNZsOll16K6upq1z733HMPVqxYgY8//hgbNmzAsWPHMG3aNNf9DocDl19+Oerr6/H999/j7bffxpIlS/Dwww8H4ikRERGRxkhCCBHoRrTl+PHjSExMxIYNGzBmzBiUl5ejU6dOeP/993H11VcDAPbt24d+/fohKysL5513Hr788kv85je/wbFjx5CUlAQAeO211/DAAw/g+PHjMJvNbZ63oqICsbGxKC8v5wKqREREOuHu329NZoLOVF5eDgCIj48HAGzfvh02mw0TJkxw7dO3b1907doVWVlZAICsrCwMGjTIFQABwMSJE1FRUYGffvqp2fNYrVZUVFQ0uRGROoQQUBTeWrrp4Ptoi4QQrpue6LHNpC5joBvQFkVRcPfdd+OCCy7AwIEDAQBFRUUwm82Ii4trsm9SUhKKiopc+5weADXe33hfcxYtWoTHHntM5WdApH9CCCjitP9CQAhACEARAgIN/1Wc9zW3L7VNAiBJEgyyBFkCZFmCLDX8W3L+u/E+SZJ82pbG18+hCCii8eb8WZzx7zNfXqnh/yRIkKRfn9evz9F5HyS47kfDPqff39xTFKe//xraCTT+2/mPM+8XZzy24X8uURYjIi2a/3NIPqD5V33evHnYs2cPNm3a5PNzLVy4EAsWLHD9XFFRgbS0NJ+fl0grqqx22OxKk8DmzD8Y5DuNf7QVR9u/cQm/BkkGSYIk49d/S2gIlpoGTI3BiyJEQwADOBoCHKH8+m9FOTuwae/zwGkBx2lbNamm3oEIs8HngSVpj6aDoPnz52PlypXYuHEjunTp4tqenJyM+vp6lJWVNckGFRcXIzk52bXP1q1bmxyvcfRY4z5nslgssFgsKj8LIv2orXc4Ax/SPAFnJsYBAVsb+zb+bedL2zxFCNTZFISbDYFuSsgRQgQ0+NRkTZAQAvPnz8eyZcuwdu1aZGRkNLl/+PDhMJlMWLNmjWtbTk4OCgoKMGrUKADAqFGjsHv3bpSUlLj2WbVqFWJiYtC/f3//PBEiHVEauj0o+DR2XVLLqus5fYq/VVntUAL8vtRkJmjevHl4//338dlnnyE6OtpVwxMbG4vw8HDExsZizpw5WLBgAeLj4xETE4M77rgDo0aNwnnnnQcAuPTSS9G/f39cf/31eOqpp1BUVIQ///nPmDdvHrM9RM2wKUqgm0AUMA5FoM7mQJiJ2SB/qLcrqLbaER7g37cmh8i3lBp76623cOONNwJwTpZ47733YunSpbBarZg4cSL+8Y9/NOnqys/Px+23347169cjMjISs2fPxpNPPgmj0b3Yj0PkKZRUW+2osvLbMIUuoywhIYpfkn1NCIGT1fVwKAIdoywwyOp3h7n791uTQZBWMAiiUFJeY0Od3RHoZhAFVFyECRYjs0G+VFFnQ22981oT6CBIkzVBROR/7A4jAmqs/CLgS3U2hysA0gJN1gQRkX+JhiHT1JTV7sAvx6uxr6gSq/cW4+djFRjUORb9U2NgMsgwGSQY5Yb/GmQYZanJdqPB+XPj9uZ+bvx3hNkAo4HfSwOt3qHA5lBg4muhOkURqKhrayyjfzEIIiLY3JiXJthVW+04UFKFnKJK5BRXIqeoEnknqs8KDrfklWJLXqnq55cAXNCzI/561UAW5wZYtdWOuIi2l1ai9qmos2lulCKDICKCPcS6wsprbdhfVIl9DcHO/qJKFJTWNDudX2y4CX2SoyGEwM/HKtA/NQbdEiJhcyiwKQIOh2j4twK7Q8De8LNdafivQ8CuKLA17udQ4FCE62e78uuMx5tyT2DG65txx8U9cXHfRE7eFyBWuwK7Q2FmTkW19Q5Y7dq7zjAIIqKgzgSdqLJiX9GvwU5OcSUKy+ua3bdTtAV9k6PRJykavZOj0Tc5GonRFp8GI0II/Hf7Efz72zw4hEBheR3+tGwPhqbFYcGlvdE7Kdpn56aWVdc7EBvOIEgNdoeCSo11gzXi6LBWcHQYhYqTVVZXRkKvREMAsa8h2GnM8pRW1ze7f5cO4U2Cnd5J0YiPDGwXSJ3NgXc35+M/Wfmw2hVIAKaek4q5F/UIeNtCjQQgwUcjl0JNaXU9bI7ms0CBHh3GIKgVDIIoFAghcLzSquGVndpWXFGHG9/aitLqs79tyhKQnhCJPqcFO72TohAdZgpAS91TXFGHl9fm4pufnUv9RFoMmDM6A78dkcaCXT+KMBs0/T7RgyqrHdWtzD/GIEjDGARRKLA5lBazJXrxxOc/Y8WPhQCcE95dPjgFvZOi0Sc5Gr0So3RbaJx9uAzPrdqPfUWVAIC0+HDcPb43LuiZwHohP5Dg/CMtMxvkEXeuLQyCNIxBEIWC2nqH5oattkfByRrMeH0zHEIgIdKMmy/MwLRhXdp+oE4oQuDzXYX4x/qDrj8o53WPx90TeiOjY2SAWxf8Ii1GRFlYPttep88K3ZpAB0HMqxKFOL1Pkvj6t7/AIQRG9+yIL+66MKgCIACQJQlThqTi47mjcMOodJgMEjb/UopZ/9qCZ1ftR0WtfgNYPaipt4O5gvartNp1MfcYgyCiEGfX8ciw/cWVWNVQN3PbRd0D3BrfirIYMW9cTyy95TyM6d0RDiHw4Q+HcfVrWfhk+5GQm+bAX4QAam3ameFYD7Q2K3RrGAQRhTh7C6M29OD1jb8AACb0SwyZoeRp8RFYfPUQvDRzKLp3jER5rQ1PfZ2D2W/8gG2H1J/EkYBqq4PZIDdpcVbo1jAIIgphdoei21Fhu4+W49sDJyBLwC0XBncWqDnnZsTjnZvPxX2X9kZMuBG5x6sw7/2deOC/u3D0VG2gmxdUFCFQZ9PvlwV/0uKs0K1hEEQUwvQ8N9A/NxwEAEwelIJuIVogbJRlXDMiDf+dez5+O6ILDJKE9fuP49rXs/CP9bmtDk2m9qmu5++yLVqdFbo1DIKIQlhLE5hp3bZDpfjh0CkYZQk3j87wyzktRu1eLmPDTbj30j549+ZzcW5GPGwOgbe/z8c1r2Xh812FUPT01VyjHIpAHWuDWuRQhGZnhW6Ndj/VRORzeiyKFkLgtQ3OWqArh3ZGaly4z89pkCXERZgRG26ClmeM6d4pCi/OOAeLrx6MLh3CcbK6Ho+v/BlzlmzD3sKKQDdP92p0UuwbCOW1Nl12rTMIIgphehwe/93Bk9h9tBwWo4zfX9DNL+eMDnPOExNmMiBG44GQJEkY07sTlt5yHuZf3BMRZgN+LqzA79/6Ae9tzg9083TN5lBgtTMQOlOV1a7brDKDIKIQpShCVwWMgLNA9bX1zlqga0Z0Qccoi8/PaTHKsBh/nXE6zGRAbIS2AyEAMBtlXH9eOv47dxSMsgQB4K3vDgW6WbpXY2UQdDqbQ0GNjmvPGAQRhSg9ZoHW7i3BgZIqRJgNuOG8bj4/nwQ0O1uwxaiPQAhwLgI6oX8SAECSoIsJ7LSs3qHoNuuhNiGEbrvBGjEIIgpReqsHsiuKa16g353bFbERvl/YMtxsgLGFBUstRgPiIsy6CIQWTu6LmHAjKurs+P7giUA3R/eYDXLSy6zQrWEQRBSi9BYEfbWnCPmlNYgJN2JmZlefn0+Sms8Cnc5slNEh0gytr2UaZjJg6pBUAMDH244EuDX6V2d36HqSUTVY7fqZFbo1DIKIQpSeusNsDgX//jYPAHDDed38sqBltMXk1krtJoOMDhHaD4SmD+sCCcCWvFLkn6wOdHN0rzoIAgBPKYqzGywYMAgiCkFCCF2lsT/LPobC8jokRJpxzQjfL5BqMsgINxva3vG0/eMjzJA1HAmlxoVjdK+OAID/bmc2yFtWmwOKjj5DatLbrNCtYRBEFIJsOuoKq7M58NZ3zizQ7y/ohjCT+8GJpzzJNBkNMjpEmDQdCDUGkCt3FXI2aS8JhOYs0nqcFbo1DIKIQpCeVhz/ePsRnKiqR0psGK4c2tnn5wszGmD2cHZoo0FGfKR2M0Iju8UjPT4CNfUOfLmnKNDN0b3a+tDKBul1VujWMAgiCkF6yQRVWe34T9YhAMCc0RkwtTBSSy0SgKgw7+qNDLKE+EgzDLL2AiFZklzZoI+3HebK6F4SAGpDaCkNvQ+Hbw6DIKIQpJeRLR9sLUBFrR3p8RGYPCjZ5+eLtBhVCV4MsoT4CG0GQpMHpSDCbMChkzXYduhUoJuje9X19pAIJqt1PCt0axgEEYUYvRRFl9fY8N6WAgDALWO6wyj79nJlkCVEtKMYui1yQyBk1FggFGUx4rJBKQCcXY3kHSGCPxtkcyhBW0PGIIgoxNgVoYuU9jub81FT70CvxCiM75fo8/NFWYxuDYlvD1mW0EGDgdDVw51dYt8eOI7C8toAt0b/qoN48sRgmBW6NQyCiEKMHiZJPFFlxUfbDgMA5l7Uw+eFxmaD7LNRZ3JDjZCv65naI6NjJEZ26wBFAJ/uOBro5uieIgTqgjAbJIRARZ3+Z4VujXY+lUTkF3qYJHHJd4dgtSsY2DkGF/RM8Pn5or0shm6LJEnoEGGCWUOB0DUj0gA452AKxj/g/hZs3UV2h4LS6vqgf29o5xNJRH6h9UzQsbJaLNvpzE7cflEP1buoztTa+mBqkiQJcRoKhEb37IjkmDCU19qwem9xoJuje3YleLJBNfV2lFbXwx7EGaBG2vg0EpHfaH1k2Bub8mBXBEakd8CIbvE+PZckAVFm3y/B8ev5nIGQxcN5iNRkkCVMH+6cd+mjbUdCYoSTr9XofCkNhyJwqroelXX2oK0BOlPgP4nN2LhxI6ZMmYLU1FRIkoTly5c3uV+SpGZvixcvdu3TrVu3s+5/8skn/fxMiLTF7lA0fXHLP1mNL3YXAgDmju3h8/NFWYyQ/Vy0LEkSYsNNCDP6fubrtkwdkgqzQUZOUSX2HKsIdHN0z+ZQUK/T2ZTrbA6crLaiXuNfktSmySCouroaQ4YMwSuvvNLs/YWFhU1ub775JiRJwvTp05vs9/jjjzfZ74477vBH84k0S+vp7dc3/gJFOLtqBnWO9em5jLKECD9mgU4nSRJiIwIfCMVFmHHpgCQAzskTyXs1OltKQ1EEymtszhFg2r48+ERgrgBtmDx5MiZPntzi/cnJTSdN++yzzzBu3Dh07969yfbo6Oiz9iUKZVqe7Gx/cSVW7y0BANx2Ufc29vZedJjJ5+doS2yECahFQGtJrhnRBSt3FWLN3hLcNd6KhChLwNoSDKx2BTaHoqnRgC2x2h2oqLVDCcXop4H2X6U2FBcX4/PPP8ecOXPOuu/JJ59EQkIChg4disWLF8Nubz1Ct1qtqKioaHIjCiZaLop+feMvAIAJ/RLROynap+fyZn0wtcWGm9q1Yr3a+ibHYFDnWNgVgeXZxwLWjmBSo/F5g4RwrgFWVmML6QAICIIg6O2330Z0dDSmTZvWZPudd96JDz74AOvWrcNtt92Gv/3tb/jjH//Y6rEWLVqE2NhY1y0tLc2XTSfyO60Oj999tBzfHjgBWQJuHePbLJAa64OpLSbMpOps1e3VuJ7Ysh1HNV84rwd1dodm59axORScrK7XfRG3WnQfBL355puYNWsWwsLCmmxfsGABxo4di8GDB2Pu3Ll45pln8NJLL8FqtbZ4rIULF6K8vNx1O3yYfeQUPBRFaLbP/7X1BwEAlw1KQXpCpE/PFaHS+mBqiw4zIcoSmODs4r6JiI8043iVFetzjgekDcGmWoO1QdVWO05V12s2QAsEXQdB3377LXJycnDzzTe3uW9mZibsdjsOHTrU4j4WiwUxMTFNbkTBQqtZoG2HSrEt/xSMsoQ5ozN8ei5ZkhAZwIxLWyItRp9P3Ngck0HGVUOdw+W5npg66uodUDQSbDgUgdLqelRZQ2fou7t0HQS98cYbGD58OIYMGdLmvtnZ2ZBlGYmJvl+DiEiLtFgPJITAqxucWaArh3ZGaly4T88XHab++mBqizAbEROAou2rhnaGQZaQfbgM+4sr/X7+YCOgjWxQbb0DJ6usmh4UEUiaDIKqqqqQnZ2N7OxsAEBeXh6ys7NRUFDg2qeiogIff/xxs1mgrKwsPP/88/jxxx/xyy+/4L333sM999yD6667Dh06dPDX0yDSFC0GQd/lnsSeoxWwGGX8/oJuPj2XyYfrg6kt3GxAbLgJ/gzXOkVbMK5PJwDAf5kNUkWtzRGwSSgVRaCsph4VdcG7+KkaNBkEbdu2DUOHDsXQoUMBOOt7hg4diocffti1zwcffAAhBGbOnHnW4y0WCz744ANcdNFFGDBgAP7617/innvuweuvv+6350CkNVrrDlOEwGsNWaBrRnRBRx8PzQ5EN5M3wkwGxEb4NxBqXE/sqz1FqKi1+fHMwUmIwMwiXWdz4ES1FVadTtzoT5LgXOktqqioQGxsLMrLy1kfRLomhEBJZcuDAgJh9c/F+L/lexBhNmD5Hy5wzpnjI2EmZ2ZFj+rtCspq6/1S1C6EwPVvbMWBkircOb4nZmWm+/6kQU6SgEizEbIkQZYBgyTBIEs+6ZYVQqDSaketjkZ+dYyy+GSggrt/vzWZCSIiddk01hVmVxTXvECzMrv6NACSJCA6QKOu1GA2yugQYYY/SpkkSXINl/9k+1GOIlKBEECV1Y6Khnl5TlbXo6TSipLKOpyssrq6rKobgher3eFc3qadUW+93Tn0XU8BkBbo98pARG6za6wr7MvdRcgvrUFsuAkzzu3q03MFYn0wtZkMMuIjzDjlh8ntJg5Ixktrc3G0rBZZv5zE6J4dfXq+UCUEYBei1aVsJOnXzJEsSzBIUrMZpSqrHdXWwBdh6xEzQUQhQEuZoHq7gjc25QEArh+V7tO5cQwBXB9MbUaDjPhIs8/nOAozGTBlSCoA4L/bWCAdSEI41/uz2hXU1juazyhV1DEA8gKDIKIQoKVZgD/LPorC8jokRJpxzfAuPj2X3oqh22KQJcRHmGH0cSB09bAukABk/XISBSdrfHou8o52vt7oE4MgoiAnhNBMbUedzYG3vjsEAPj9Bd18OmTdYpRhCfAq7b4gyxI6RJh9ukBn5w7huKChG+y/O5gNouDFIIgoyNkVoZlvix9vP4KT1fVIiQ3DlQ0zFPuCBARsCQp/cAZCJph9GAg1Fkiv3HUMNRqY9I/IFxgEEQU5rUySWGW14z9ZhwAAN1+Y4dNMRrjZAKMPj68FkiQhLsIEi9E3z/PcjHikxYej2urAV3uKfHIOd7y09gDGPb0ebzbUkRGpKbivEkSkmUkSP9hagIpaO9LjIzBpYLLPziNLUlBngU7nDITMCPNBt58sSbhmuHPyxI+3HQnIzMcb9h/Hu5sLUFPvwL835bEAmFTHIIgoyGkhE3S80ool3x8CAAxJi4NR9t2lJ8qi/fXB1BYbYUK4DxaGvXxQCsJNBvxyohrb80+pfvzWLN95FA9+ssv1s0MR+OvnewO2DAUFJwZBREFOCyPDXlp7wDVMf2veSZ+dx2SQfRIM6EFMmAkRKj/3qDAjLhvkzNr5a3V5IQTe2JSHRV/ugyKAqUNS8c/rh8MoS1izrwRLtx72SzsoNDAIIgpidocS8KLo7fmn8PVPxQCAhEgzZp/fzWfnCpVusJZEh5lU/x1c3TCNwcb9x1FUXqfqsc/kUAQWf53jmk389+d3w58u64tz0uJwzyW9AQAvr83FDj9npSh4MQgiCmKtzUbrl/M7FCz+OgcAMH1YZ3xx14WYNsw3cwOZDDLMPioS1pNIi1HV+ZG6d4rCiPQOUATw6U7fZYOsdgf+vHwPPtlxFBKA+y7tjblje7i6NqcP64zJA5PhEAJ/WrYbJZW+DcgoNPCKQRTEbAHuCvtw22HknahGXLgJt13Uw6fnMhpCqw6oNRFmo6oLxjYOl/9s5zFY7eqvTVVVZ8fdH2Rj7b4SmAwSnrhyoGtF+0aSJOHByX3RKzEKp2ps+NOnewL+/ib9YxBEFMQCWRRdUlmHf3/rHNY87+KePl/F3dezKOtNmMmA2HAT1PitjO7VEUkxFpTV2rBmb4kKR/zViSor5r67HTsKyhBhNuD5a8/BhP5Jze4bZjLgyemDEB1mxO6j5Xhh9QFV20Khh0EQURAL5PD4F9fkoqbegYGdY/CbwSk+P5+v19TSozCTAbER3gdCRll2dWN+rOJ6YgUna3Dz29twoKQK8ZFmvHbdcIzoFt/qY7p0iMCjUwc427L9CL7cU6haeyj0MAgiClIORSBQo4l/yCvFqp+LIUvAHyf2heyHIeu+HHavZxajAXERZq+Pc8WQVJgNMn4urMCeo+VeH+/nYxW45T/bUFhehy4dwvHvG0agT3K0W48d3bMj5ozOAAAs+mIf9hdXet0eCk28ahAFKXuAskA2h4Knv2kshu7i9h82b0hgJqg1ZqPsdXdhh0gzLmnopvJ2uPzmX07iD+/tQFmtDX2To/GvG0agc4fwdh1jzugMjOqeAKtdwYOf7EZFrc2rNlFoYhBEFKQCVQ/0wdbDOHSyBh0iTLjtou5+OScDoLaZVBg511ggvWZvMU5WWT06xld7irDgox9Ra3Pg3Ix4/GPWMMRHtj9TZZAlPHbFAKTEhuFoWS0eW/EzFE6kSO3EIIgoSAUiCCquqMMbDWs83XFxL0SH+bYYuhG7wtqmxmKr/VJiMCA1BjaHwP9+PNbux7+/pQCP/O8nOBSBS/sn4dnfDkGkF/MaxYab8Pfpg2ExytiUewJvfXfI42NRaOKVgyhIBaIo+oXVB1Brc2Bwl1hMHuS79cHOZODw+DaptWBtYzbokx1H3e5yVYTAS2sP4IU1ztFcM0am4bErBqjSpj7J0fjjpD4AgH9t/AVZB303IzkFHwZBREFICAGHnydK3JJ3Emv2lUCWgPsn9vFLMXQjDo9vm0GWVHlNxvdNQocIE45XWrEh53ib+9sdCh5f8TPe3VwAAJg/rifuntBL1ffHbwan4qqhnSEAPPzZHhwrq1Xt2BTcGAQRBSGbn7vC6u0Knv56PwDnMgu9k3xfDH06BkHuUaNLzGyUceXQzgCA/7ZRIF1b78B9/92FL/cUwSBJePg3/XH9qHSfLHC74JLeGJAag4o6Ox78dDfqbOpP6kjBh0EQURDy98iw97cWoKC0BvGRZtw2xrczQzeHhdHuMRnV+T1dNbQzDJKEHQVlyC2panafspp6zHt/B7IOnoTFKOOpawbjch/OF2U2ylg0bRDiwk3IKarE4q9zuOI8tYlBEFEQ8mcmqKi8Dm82FEPfOb4nolRct8odBlnySWYhGKlVF5QUE4aL+nQC0Hw26FhZLW75z3b8dKwCMeFGvDJrGEb37KjKudtq1xNXDoQsASt3FWJ5dvuLtym0MAgiCkJ2P66p9Nzq/bDaFZyTFodJA/xXDN2IXWHuMxlkVZbRAIDfNhRIf7mnsMkcPQdKKnHLf7ahoLQGSTEWvH79CAzqHKvSWds2MiMet491ZiOf+SYHPx3zfmJHCl4MgoiCjD+LorMOnsT6nOMwSBLun9gnIBkZdoW1j1rZoHPS4tCzUxTqbAo+3+1cumJH/inMfWcHTlTVo3vHSPzrhhHI6Bipyvna4/rz0jG2dyfYHAILP92NU9X1fm8D6QODIKIgY1cE/BEC1dt/nRn6tyO7oGdilB/OejbOEdQ+akyaCDhXdb+6IRv03+1HsHZfCe76IBtVVjuGdInFP68fjqSYMFXO5UnbHprSH13jI1BcYcWfl+8J2AzqvnSquh63vbMdk1/4Fp/uUG9Nt1DCqwdRkPHXJInvbcnHkVO1SIg04+YL/TMzdHOYCWofk4pzKk0akIzoMCOOnKrFwk93o96hYEzvjnhx5lDEhPtnosyWRFmM+Pv0QQg3GbAt/xT+ueGXgLbHF55ffQDZh8tQWl3PiSI9xCCIKMj4Y5LEY2W1rovuXRN6IcqLWX+9xZqg9lFjmHyjcLMBUwanun4e0iUWi6YNQpjJoNo5vNG9UxT+7/J+AID/ZOVjfU5JgFuknp+OleOrn4pcPyfHBibrpncMgoiCjD8yQY3F0MO6xuHShkU1A0GSAJlBULtIkqRq4Pi7zK6u7FJxRZ3muicv6Z+EmeemAQAeW/Ez8k9WB7hF3hNC4PnVztm3G4vOdx0px478U4Fsli5p691KRF7z9ciwTbknsHH/CRjkwBVDN9LaH1y9UKsuCAA6RVuw4JLeSI4Jw+zzu6l2XDXNH9cTQ9PiUFPvwAOf7EZNvT3QTfLK6r0l2HWkHGEm59xIVzVMXrnoy32w2jlJZHvwCkIUROwOxadF0Va7A89+45wZesbINHTvFJhi6EasB/KMml1iADBtWBd8Nv8CTBvWRdXjqsVokPHXqwaiY5QZeSeq8dfP9+p2IkWr3YFX1uUCcI6C6xRtwbxxPdAxyoyC0hrWBrUTgyCiIGL38dD4d7LycbSsFp2iLJgzOsOn53IH64E8o9YweT1JiLLgb1cNgkGWsHpvCT744XCgm+SRpVsPo7C8DonRFlx3XjoAIDrMhHsvdS4i+5+sfBxsYRZvOpsmPwkbN27ElClTkJqaCkmSsHz58ib333jjjZAkqclt0qRJTfYpLS3FrFmzEBMTg7i4OMyZMwdVVXxjUHCz+bAr7OipWvwnKx+Asxg6MoDF0I2MXD3eI2otpqo3Q9LicPf4XgCAl9bkYmeBvmpoTlZZ8fb3hwAAfxjXo0kB+rg+nXBhr45wKAKLvtwHRaeZLn/TZBBUXV2NIUOG4JVXXmlxn0mTJqGwsNB1W7p0aZP7Z82ahZ9++gmrVq3CypUrsXHjRtx6662+bjpRQPmyKPrZVc5i6BHpHTChX6LPztMerAnynNpdYnpxzYgumDQgGQ4h8Kdle3C80hroJrntnxt/QU29A/1TYjDxjNnZpYYJSyPMBuw+Wo5P2ljclpw0+SmYPHkynnjiCVx11VUt7mOxWJCcnOy6dejQwXXf3r178dVXX+Hf//43MjMzMXr0aLz00kv44IMPcOwY15Kh4OWr4fHfHjiOTbknYNRAMXQjCawJ8oZai6nqjSRJeHByX/TsFIXS6nr8adlun2ZQ1bK/uBL/a1gL7e4JvZrN5CXFhOEPDUuG/GP9QRRX1Pm1jXqkySDIHevXr0diYiL69OmD22+/HSdPnnTdl5WVhbi4OIwYMcK1bcKECZBlGVu2bGnxmFarFRUVFU1uRHrhUAR8kQGvsznwTEMx9O8yu6JbAJZBaA4DIO+EYl1Qo3CzAU9OH4QoixG7jpTjxTUHAt2kVgkh8MLqAxAAJvRLxJC0uBb3nTasCwZ2jkFNvQNPf5Oj2wJwf9Hlp2DSpEn4z3/+gzVr1uDvf/87NmzYgMmTJ8PhcA4NLCoqQmJi03S90WhEfHw8ioqKmjskAGDRokWIjY113dLS0nz6PIjU5Ktvs//JyncVYv7+gm4+OYcn2BXmHTUXU9WjtPgIPDq1PwDgo21H8NfP9wa4RS379sAJbMs/BbNBxrxxPVvd1yBL+NPkfjDKEjbuP4F1Ocf91Ep90uVVZMaMGZg6dSoGDRqEK6+8EitXrsQPP/yA9evXe3XchQsXory83HU7fFifowcoNPliZNjh0hq801AMffeEXogwB74YupGBRdFeC+VsEABc2KsTIi3O4uIVPx5DQWlNgFt0NptDwYtrnZmqGeemITUuvM3H9EiMwvWjnCPHnvkmB5V1Np+2Uc+C4hPQvXt3dOzYEbm5zrkTkpOTUVLSdHp0u92O0tJSJCcnN3cIAM46o5iYmCY3Ir1Qe5JEIQSeWbUf9Q4F52bE4+K+2iiGbsTh8d5Tc9JEvbp9bA+YDBIEgD99uht1Nm1NNvjf7UdwuLQWHSJM7ZqM8vcXdEPX+AicqKrHK+sO+q6BOhcUn4AjR47g5MmTSElJAQCMGjUKZWVl2L59u2uftWvXQlEUZGZmBqqZRD6ldiZo44ETyDp4EkZZwn2X9tZEMfTpWBPkPTUXU9Wra4anYdkfLkCHCBMOlFS5lqPQgvIaG97YlAcAmHtRj3at0WcxGrBwcl8AwLKdR3U3HYC/aDIIqqqqQnZ2NrKzswEAeXl5yM7ORkFBAaqqqnD//fdj8+bNOHToENasWYMrrrgCPXv2xMSJEwEA/fr1w6RJk3DLLbdg69at+O677zB//nzMmDEDqamprZyZSJ+EEHCoGATV2Rx4bpWzGPq689KRnqCNYujTMRPkvVAdJn+mTtEWPDp1ACQ4A4avf2q5dtSf/r3pF1TW2dEzMQpThrT/b9ew9A644hzn4578ch/q7dofBedvmvwEbNu2DUOHDsXQoUMBAAsWLMDQoUPx8MMPw2AwYNeuXZg6dSp69+6NOXPmYPjw4fj2229hsVhcx3jvvffQt29fjB8/HpdddhlGjx6N119/PVBPicinbCrPD7Tku0MoLK9DckwYbtTgelBywySp5B21F1PVs/O6J+DGhsL/J7/cF/CFVg+dqMYn248CAO4e38vjzOf8cT0RH2nGoZM1rokWtUTNL2+ekATHz7WooqICsbGxKC8vZ30QaVpNvR2VdeosCllwsga/+/dm2BwCf58+CGP7aKsWCHBmMDpEmgPdjKBQUWdDbb226mACxaEIzH9/B3YUlKFnYhTemD2iyazM/rTgo2x8l3sSF/bqiKevGeLVsVb/XIz/W74HRlnCO3PODfiaf4Aze33/x7uw9VAp/u+yfrhB5S9b7v791mQmiIjaR61MkBACT3+TA5tDYFT3BFzUu5Mqx1Ubl8tQD7vEfmWQJfzlyoHoEGFCbkkVnm3oEva3LXkn8V3uSRhkCXde3Mvr443vl4jRPTvCrpElNWwOBX9ZuRff5p6A1a7g+QDO08R3P1EQUGtk2Pqc49iSVwqTQcICDRZDN+IcQeoJ9WHyZ+oYZcFfrhgICcBn2cfw1R7/1gfZFQUvNBRnXz28C7omRHh9zNOX1Nh1pBzLdx71+pieKq+14c6lO/H57kJIAGLDTVhwSe+AtYfvfiKdU6sourbegedW/1oM3TXe+4uvr3BkmHpCdTHV1ozMiMec0RkAnPVBh074rz7of9nHcPB4NWLCjK42qCE5NgxzL3IuqfHyulyUVPp/SY3DpTW4+e1t2FFQhgizAc/POAc7HroE152X7ve2NGIQRKRzdkVAjeT2n5btRnGFFTFhRk0WQ5+OxbzqYpfY2W4anYER6R1Qa3PgT8v8M39QVZ0dr2/8BQBwy4XdERtuUvX4Vw/vggGpMai2/roUjr/8eLgMc97ehoLSGiTHhOFfN4zAed0T/NqG5vCdT6Rzaq0cvzWvFIAzdR6oYlB3SBIgMwhSVaguptoagyzh8SsGID7SjIPHq/H0Nzk+P+eS7w/hVI0N3RIiMG1YZ9WPb5AlLLysLwyyhPU5x7HBT0tqfP1TEea9vwPltTb0S4nGGzeOQM/EwBdnAwyCiHRPjZXjbQ7FVSw581xtr5nHeiD1sS6oeQlRFvzligGQJWDFj4X4Ynehz8519FQtPvihAABwx/heMProNemVGI3rzusKAFj8dQ6qrOqMKm2OEAJvbMrDw5/9BJtDYGzvTnh11nB0jLK0/WA/4TufSOfUyAT9crwaioAuusJYD6S+UF9MtTUjuv1aH/T3r/bhl+NVPjnPS2sPwOYQODcjHhf08G030U0XZKBLh3Acr7LiH+tyfXKOeruCx1f+7Orem5XZFYumD0K4WVtZZgZBRDqnxsiw/cWVAIBeSdGaHRHWiPVAvsFsUMt+f0EGzu0Wjzqbgj8t26P6vEo7C05hXc5xyJJzYkRffwbDTL8uqfHpjqPYdaRM1eOX19pw1wc78cXuIhgkCQ9M6oM7x/fSZAE+3/VEOmZ3KKoURTcGQX2SolU4mm8xE+QbXEy1ZQZZwqNT+yMh0oy8E9VYrGJ9kCKEa72yK87pjB5+qpUZ0S0evxmcAgHgb1/sg02laTbOHAH27LVDMG1YF1WO7Qt81xPpmFqLpu4vdqb4eyVpo1ixNcwE+QYXU21dQpQFf7lyIGQJ+HxXIVbuOqbKcb/cXYR9RZWItBhw65juqhzTXXde3AsdIkzIO1GN/2Tle328bI2OAGsNgyAiHVPj25sihCsT1FvjmSAJ8FnBaKjjMPm2DU/vgFsudAYqT32V43V9UG29A6+uPwgA+P35GYj381IwsREm3NMwUeFb3+V5NR/S1z8VYb5GR4C1hu96Ih1Toyj6WFktauodMBtkdFNhdlpf4tB43+Fiqu658YJuODcjHla7goWf7vaqPuidzfk4XmVFalwYrh0ZmFGZl/ZPwqgeCbA5PFtSQwiBf3/7i6ZHgLWGQRCRjqkxPD6nyJkF6t4pUvNZFhOHx/sU64LaJksSHps6AJ2iLDh0sgZPfb0PnqxDXlxRh3c3O7ug5o/rCXOAfveSJOGPE/sgzCQj+3AZ/pftfjdfvV3BYyt+xr++zQOg3RFgreE7nkinHIqAGusgHmioB+qTrO2uMAAwsG7Fp9gl5p74SDP+cqVz/qAvdhdhxa72zx/0j/UHYbUrOCctDhf3TfS4LWp8IlLjwl1Lary0NhcnqqxtPqZxDbAv92h/BFhr+I4n0im1RnPsL2kYHq+D/nt21/gWh8m7b2jXDrhtjDNwePrrHOSWuF8f9NOxctfCrHdP8G5IvEWl2d1/OyIN/VKiUWW149k2ltQ4XFqDOW//gJ2H9TECrDVev+PtdjuKi4tRUFDQ4o2I1KfayLAiHWWCGAT5FBdTbZ8bzk/Hed2d9UH/t2w3aurbnn1ZnDYk/rJByeiXEuNVGyJV6noyyBL+dFk/GCQJa/aV4NsDzS+p0TgC7HBprW5GgLXG4yBo9erVGDt2LKKiopCamoqMjIxmb927+3fIH1GoUGOSxNLqehyvskICdDGSg5kg32OXmPtkScKjUwagU7SzPujvX+W0WR+0Zm8Jdh0pR5hJxu1je3h1fqMswWiQVQtceydF43eZziU1nvrq7CU19DoCrDVGTx60cuVKXHXVVXA4HOjQoQMyMjIQHa39b5FEwcSmwsiwxqHxafERiDB7dDnwG1mSND+bdTAwGSXU+W45qaDTIdKMJ64ciD+8uwNf7SnCsK5xuOKc5hc/tdodeLlhmYrrz0tHYnSYV+du7AozyhLqVVpI+eYLM7B2XwmOltXinxsO4t5L+7jWAGssgB7buxMenTpAVwXQLfHoqvfYY49BURQ899xzmD9/PgwG/f8iiPREUUS7h7I2p7EoujcnSaQGrAtqv3PS4jB3bHe8su4gnvlmP/qnxqBX4tmJgQ+2HkZheR06RVswKzPd6/NaGkaUGQwSoNJKHmEmAx6Y3Ad3Ls3Gx9uO4OK+ifgs+xi+bKhhmpXZFfMv7hk03aYevdt/+uknjBo1CnfddRcDIKIAUKseKEcnkyQCHBnmLyaDjCD5++ZX152XjvN7JMBqV/CnT/eg+oyupJNVViz5/hAA4A9je3idRZElyRWwqv0FITMjAZcNSoYAMO89/Y8Aa41HQVBUVBS6du2qdluIyE12FeYHAoADOgqCmAnyH87H1H6yJOGRKf2RGG1BQWkNnvyy6fxB/9z4C2rqHeiXEo1JA5O9Pp/F9OtrZPTB63XX+F4IN8lwCAGTQdL1CLDWePSbmzBhArZt26Z2W4jITWrUA9XWO5B/sgaAPrrDODLMfzhpomfiIpz1QQZJwjc/F2N5w8SDB0oqseJH57/vntBblUzK6QXsvviCEBdhRnhDnWBsuEnXI8Ba49E7/e9//zsqKirwwAMPwG5nBR2Rv6kxMiz3eBUEgIRIMxJ0MMW9L77tUvO4mKrnhqTF4fZxzlFfz36zH/uLK/HC6gNQBDC+byLOSYvz+hwSfq0HApzLyfiih+qWCzOQHBOGOaMz1D+4RnhUGP3WW29h8uTJePrpp/HJJ59g7Nix6NKlC+RmLlKSJOGhhx7yuqFE5CSEgEOFmqD9Dctl9NbB/ECSxEyQP3GYvHdmZXbFzoJT+C73JOa/vxPltTaYDTLmX9xTleObjfJZIyWNsqzaBKqNpg3rEpRdYKfzKAh69NFHIUkShBD45Zdf8Msvv7S4L4MgInXZFQE1yqJ/XTle+11hzAL5V+NiqmoV4IcaWZLwyG8G4Po3t6C4wrkExdCucUiNC1fl+M2tM2Y0SLCpNEIslHgUBL355pucr4MoQNRYOR4A9jcOj29mKK/WMAvkfyajDLsXK6SHutgIE/561SDc/LazfvbQyWrVjm0xnj2yjAMHPONREDRt2jRIksQJEokCQI2V4+2KgoPHG4IgHXSH8QLvf2aDjFq1Jp8JUYM6x+K2Md2xbOdR3Hh+N1WOaZSlZr8U8IuCZzwKgjp06IBzzz0XWVlZareHiNqgRiao4GQNrHYFEWYDunRQJ0XvS7zA+x8nTVTHTaMzcJOKhcUtLZjKLmPPePRbi4mJ4ZpgRAGixsiwxq6wnolRupj4jJkg/+NiqtpkaWH6AoMsga9W+3kUBA0dOhQHDx5Uuy1E1Aa7Q1GlKFpPM0VLAIzMSgQER4lpiyS1nqHj56T9PPqNPfDAA/jhhx/w3//+V+32EFEr1Bqt0zhTdB8dBEEys0ABYzLyd68lzRVEn47dxu3nUU1QeHg4br75Zlx77bX4zW9+gylTpqBr164IC2t+RdwxY8Z41UgiclIjCBJCuLrDeulieDwv7IHCuiBtaakrrBE/K+3nURA0duxY1zxBK1aswMqVK1vd3+HgCAMiNagxSWJJpRXltTYYZAndO0Wq0Crf4rfbwGlcTFVwuqCAO3OW6Obws9J+HgVBN9xwg0/nCdq4cSMWL16M7du3o7CwEMuWLcOVV14JALDZbPjzn/+ML774Ar/88gtiY2MxYcIEPPnkk0hNTXUdo1u3bsjPz29y3EWLFuHBBx/0WbuJfE2VmaIbusIyEiLbTK9rAUe9BJZJllGv8kzE1H7OgLT1v7vM3LWfR0HQkiVLVG5GU9XV1RgyZAhuuukmTJs2rcl9NTU12LFjBx566CEMGTIEp06dwl133YWpU6eetajr448/jltuucX1M+c1Ir1TIwjKcS2Xof2uMIDfbgPNZGQQpAWnrxrfksYRYkzcuc+jIMjXJk+ejMmTJzd7X2xsLFatWtVk28svv4xzzz0XBQUF6Nq1q2t7dHQ0kpOTfdpWIn8RQkBRoV/iQONM0Tooiga4mGeg8fevDe5mbQ1c7qRdgiJ3Vl5eDkmSEBcX12T7k08+iYSEBAwdOhSLFy9uc8V7q9WKioqKJjcirVAjCwToa3i8LElcoifAOEw+8FqaJbr5ffl6tYdHmaCbbrrJ7X0lScIbb7zhyWncUldXhwceeAAzZ85ETEyMa/udd96JYcOGIT4+Ht9//z0WLlyIwsJCPPvssy0ea9GiRXjsscd81lYib6jx7a6yzobC8joAQK9E7XeHcbRL4HEx1cBrbsHUlhgMEtD69306jSRE+/PrshuRZuPoMUmSvBodJklSk8Lo09lsNkyfPh1HjhzB+vXrmwRBZ3rzzTdx2223oaqqChaLpdl9rFYrrFar6+eKigqkpaWhvLy81WMT+UNNvR2Vdd5d3bbnn8If3tuBlNgwLJ93gUot851wswExYaZANyPkVdTZUMvFVAOmQ4TZ7UCozuZAea3Nxy1ST8coi0/q/ioqKhAbG9vm32+PMkHr1q1rdruiKDh8+DC++eYbfPDBB7jnnnswZcoUT07RJpvNht/+9rfIz8/H2rVr2wxSMjMzYbfbcejQIfTp06fZfSwWS4sBElGgqfFNfL+OusIAZoK0goupBo4ktS8TxM9M+3gUBF100UWt3n/DDTfg8ssvx+zZszF16lSPGtaaxgDowIEDWLduHRISEtp8THZ2NmRZRmJiourtIfIHRdUgSPtdYQBHhmkF64ICp73TWBgNMkeItYPPRofNnDkTTz31FB599FGsXbu2XY+tqqpCbm6u6+e8vDxkZ2cjPj4eKSkpuPrqq7Fjxw6sXLkSDocDRUVFAID4+HiYzWZkZWVhy5YtGDduHKKjo5GVlYV77rkH1113HTp06KDq8yTyF3UyQfoaGcYiT22QGxZTVWN0IrVPWxMkNkeWJdUGUgQ7nw6R79WrF7766qt2P27btm0YN26c6+cFCxYAAGbPno1HH30U//vf/wAA55xzTpPHrVu3DmPHjoXFYsEHH3yARx99FFarFRkZGbjnnntcxyHSI28zQfV2BXknqgHoIwiSwEyQlpgNMurs7BLzJ3dmiW6OSZbhUPhaucNnQZCiKNi1a5dbRdRnGjt2LFqr126rlnvYsGHYvHlzu89LpFWKIrxOb/9yogoORSAm3IikGO3XvjEA0haTUYKXdfkeMxtCc8JGd2aJbg5HiLlP9VxzTU0NsrOzMXPmTBw4cKDN+iEiapsqXWFFDV1hidG6mHuHXWHaEqi6oHCzAXERJujgLas6d2aJbg6Lo93nUSbIYGi7UEsIgU6dOmHx4sWenIKITqNGLYarKDpZ+11hQMO3WdIMYwAWUzUZZERbjJAkCZFmI6qsoZXe8HRtP2ZR3edREJSWltbiN0mz2YyUlBRcdNFFmDdvHkdjEalAjUxQjs5GhvHbrPaYDTKsdv90S0kSEBtucv2tiTAbUF1vD5kV7Q3tmCX6TPzsuM+jIOjQoUMqN4OIWuPtSA9FCOSWOLvD+uigKBrghVyLTH4MgmLDTU2CgFDLBnlSEN1IkpwBFEeItY2d7kQ64O3F7OipWtTUO2AxyuiaEKFSq3yLKX3tMfmpLijSYmy2KyjCbAiZ2iBPu8Ia8UuEezx6R3fv3h0PPPBAm/stXLgQPXr08OQURHQab4OgxnqgHp2idFFwbJC5cKoWmQwSfP2qmA0yoizNd1I0ZoOCXXtniW4Ov0S4x6Pf8qFDh3D8+PE29ztx4gS7zoi8JITwujC6cZLEXqwHIi9IkgSjD7NBsiQhNrz1teJCIRtkcWPwUVv08GVHC3z6W6qurobJxMUPibyhRr9+Y1G0XuqB+C1Wu0w+GrUnwVkHJLfx2odCNsjTofGnM3J0pVt8EgQpioK9e/di3bp16Nq1qy9OQRQy1BgZdkB3C6fyW6xW+aouKCrM6HYXULBng9SYk4nZVPe4/Zs2GAyuGwC8/fbbTbadfjOZTBg4cCCKi4sxc+ZMnzWeKBR42xV2ssqKE1X1kAD0TNRHdxgzQdrli0kTw4wGRLQjuxPM2SCzQW4zG+YOSXKu90atc/tddPrcQAUFBYiIiEDHjh2b3ddsNiM1NRVTp07FnXfeqU5LiUKUt5mgAw1D47vGRyDc7H2tgT/wW6x2ybK6w68NsoSY8PYHNME6b5C3BdGnM8oS6h1B9gtSmdvvvNMLnGVZxjXXXIM333zTF20iotN4u3BqTpG+ZoqWJKjyTZh8x2RQZ4FOCUDcaRMituuxkoQoixGVgVrQzEe8mR/oTEaDhHquo9oqj/KJ69atQ3JystptIaJmeJsJ2q+7maJZD6R1ZoOMOpv3f11jwk1ejTYLNxlQbXWosqyMFhhkdUffOT9LjIJa41EQdOaiqFarFaWlpbBYLIiPj1elYUTk5G0mqHF4vF6KolkPpH1qjBALNxsQZvKue1aSJERaDEGTDVIzCwTws+QOr37jr7/+OoYOHYrIyEh06dIF9913n+u+Tz/9FNOmTUNubq7XjSQKVYoi4E0IVFNvx+HSGgBAL50URbMeSPsaF1P1+PGyhOgWJkRsr3CTIWgKgNWsBwL4WXKHR79xh8OBq666Crfffjv27t2Lfv36QZyRjhwyZAiWL1+ODz/8UJWGEoUib7vCckuqIAB0jDIjIcqiTqN8jPOb6IOno8QkCYiLMKs2I3hjNkjvJMn7pTLOJMtSUE8loAaP3sUvv/wyPvvsM0yePBn5+fnYvXv3Wfv06NEDPXv2xJdfful1I4lClVozReulKwxgTZBeeDpfUEyYSfVummDIBqkxS3RzTPw8tcqj386SJUuQlJSEDz/8EElJSS3u179/f+Tn53vcOKJQp15RtD6CIAmsY9ALT4KgCBXqgJoTDNkgNWaJbo6BmdVWefRbz8nJQWZmJiIjI1vdLzIy0q01xoioeWotnKqXkWEMgPSjvYupmg0yosN8t4yS3rNBvpiEEmBdUFs8+q2bTCbU1dW1uV9BQQGio/XxDZRIi7wJguwOBQdLqgHoJxPErjD9aM9iqu4sjKpGe/SaDTKpNEt0c/jFonUeXXEGDBiA7du3o7KyssV9SkpKkJ2djXPOOcfTthGFPG+CoPyTNah3KIgwG9C5Q7iKrfIdpu71xd2h8u4sjKoGvWaD1B4afzrWBLXOo9/O9ddfj5MnT2Lu3Lmor68/636Hw4F58+ahpqYGs2fP9rqRRKFICOFVYfT+EueXlF6JUbr5w8DUvb64UxcUZXF/YVRvNc4irTe+DII4Qqx1Hv3mb731VowdOxZLly5Fnz59MHfuXADAjz/+iLvuugu9e/fGJ598gksuuQSzZs1StcFEocLreqAi58iwPjpZLgNg6l5v2qpjsRhlRPo5KAkzyboJ+gFnV6Gas0Q3h93MLfPoN2MwGPDFF1/g9ttvx7Fjx/D6668DAHbu3ImXXnoJBQUFuOWWW7B8+XLV5oIgCjUOr4fHN2SCdFIPBDATpDeNi6k2xyBLiPFhIXRL9JYN8tWosNPxy0XLPH6nhIWF4ZVXXsGjjz6K9evX49ChQ1AUBV26dMG4ceOQmpqqZjuJQo43mSAhhCsI6qOTIEiWJH5p0qHmFlOV4L86oOaEmw2ostp1saaYL7vCGvHLRcu8Dpc7deqEa665ptn76uvr8eabb7q6y4jIfd4EQcUVVlTU2WGQJWR0bH0qC63ghVqfmltMNTrM5PFkimqJshhRUWcLaBvaIsF3Q+NPx1nYW+aT335NTQ2eeeYZZGRkYN68eb44BVHQ8yYIymnIAnXvGOm3olRv8UKtT2eOEAszGRBuDvxQ9XCzQfPdQGaj7JfsJ2uCWtauTND333+Pr7/+GiUlJUhMTMSkSZMwatQo1/3V1dV47rnn8MILL6C0tBRCCAwfPlz1RhOFAm+CoAM6myka4IVarxoXUxXCmc2LCdNOPU6kWdvZILXXCmuJQXZObKn9zkH/c/vdetNNN+Htt98G4Kw3kCQJTzzxBObPn48XXngBq1evxuzZs1FUVAQhBIYOHYpHH30UU6ZM8VnjiYKZGpmgXjqZKRpg8aaemQ0y6h0KYsNNmqrrCjcbUF1v93qkpa/4ox6okUGWvF6GJxi5FQS9/fbbWLJkCQBg0qRJGDBgACorK7F69Wq8/PLLSE5OxmOPPYb6+noMGDAATzzxBK644gpftpsoqCmK8Opb24GGhVP1UhQNsCZIz0wGGWEmg8+HensiymJEea32skG+nCW6OUZZhv2MAnZyMwh66623IEkSli1bhqlTp7q22+12XHvttfjzn/8MALjzzjvxzDPPwOCj1XCJQoU339jKa20oLHcua6OXTJAkIWAjich7EWaDpjJApwszOUeKaS0b5O9aPaNBAux+PaUuuPUq7N69GyNHjmwSAAGA0WjEE088ASEE0tPT8dxzzzEAIlKBN0N7G+uBUuPCfLpgpZpYD6RvWg2AGmlx3iB/doUB7G5uiVuvQnl5OXr16tXsfY3bR44cqfkPApFeeJMJ2t/QFdY7UT9dYbxAky+FmbQ1UkyWJL9PIcDu5ua59SooigKTqflvlEajM8KOjFRvLpKNGzdiypQpSE1NhSRJWL58eZP7hRB4+OGHkZKSgvDwcEyYMAEHDhxosk9paSlmzZqFmJgYxMXFYc6cOaiqqlKtjUS+5E3qvnGSxN46Wi6DF2jyNS1lg/wxS/SZjAYZ/JSdTZM56OrqagwZMgSvvPJKs/c/9dRTePHFF/Haa69hy5YtiIyMxMSJE1FXV+faZ9asWfjpp5+watUqrFy5Ehs3bsStt97qr6dA5BVVgiCd1AMBzASR72kpG+SPCRKbo5XnryWSEG0XH8iyjKioKHTs2LHZ+/Pz8xEZGdns/ZIk4eDBg543sKEg+8orrwTgzAKlpqbi3nvvxX333QfA2V2XlJSEJUuWYMaMGdi7dy/69++PH374ASNGjAAAfPXVV7jssstw5MgRt5f0qKioQGxsLMrLyxETE+PxcyBqr+OVVo/qgqx2B8Yt3gCHEPjf/AuQFBPmg9apLyHSrMmRRRRc6myOgI8UkwB0irYEpHykvMaGOru2Roh1jLL4JDhz9++32/nBqqqqVruTWrpf7Rc6Ly8PRUVFmDBhgmtbbGwsMjMzkZWVhRkzZiArKwtxcXGuAAgAJkyYAFmWsWXLFlx11VXNHttqtcJqtbp+rqioULXtRO4QQnhcGP3L8Wo4hEBsuAmJ0RaVW+YbEsAAiPxCCyPF/DVLdHMMHCF2FreCoLy8PF+3w21FRUUAgKSkpCbbk5KSXPcVFRUhMTGxyf1GoxHx8fGufZqzaNEiPPbYYyq3mKh91OgK65MUrZuBChwaT/4U6HmD/DVLdHNYe3c2t4Kg9PR0X7dDExYuXIgFCxa4fq6oqEBaWloAW0ShyOHF8PjGkWF6mR8IAEwcHk9+FGYyoNpqD9jsyYFcy49B0Nl0d/VJTk4GABQXFzfZXlxc7LovOTkZJSUlTe632+0oLS117dMci8WCmJiYJjcif1MlE6SjkWEGLpxKfhYZoJFiRlkKaHEyC6PPprsgKCMjA8nJyVizZo1rW0VFBbZs2eJazHXUqFEoKyvD9u3bXfusXbsWiqIgMzPT720mag9PgyBFCNdyGb0S9ZMJ4rdT8rcwkyEg7zuLKbCTCUtSYIMwLdLOxAmnqaqqQm5uruvnvLw8ZGdnIz4+Hl27dsXdd9+NJ554Ar169UJGRgYeeughpKamukaQ9evXD5MmTcItt9yC1157DTabDfPnz8eMGTPcHhlGFCieBkFHSmtRa3PAYpSRnqDevF2+xosyBUJkAGqD/D1LdHOMsqS5JUQCSZNB0LZt2zBu3DjXz411OrNnz8aSJUvwxz/+EdXV1bj11ltRVlaG0aNH46uvvkJY2K/Dgd977z3Mnz8f48ePhyzLmD59Ol588UW/Pxei9vL0AtXYFdYzMUpXgQUzQRQI/q4NCsQs0c3R07XBH9yaJyhUcZ4gCoSSijqPVpB/ZV0u/pOVj6uGdsaDk/uq3i5fkCUJnXQylJ+Cjz/nDQozGRAbHvi1/LQwV9LpAj1PUODDUiJyURThUQAEwFUPpKeZopkFokDyZ22QFrrCAGaCzqSNV4WIAHi3cGpOQ3dYrySODCNylz9GiknQThDELx5NabImiChUeTpT9MkqK0qr6yFLHBlG1B5hJgMcioBdEVAUAYcQXmVkmxPIWaLPJEkSZEny+FoTbBgEEWmIp5mgxixQ1/gIhAV4GG57MDVPWtBcNqgxIHIozmVsHIqAosC1TQj3A6VATpDYHJNBgtXOIAjwYRDUv39/5OTkQJIk2O1crITIHZ6PDGusB9JPVxgAGDlbNGmULEuQIaG17xQO5YwgyfVfuH4GArtURnP45eNXPguCFEVxRspMuRG5TfE0CCpyZoL0FARJEi/GpG+GNmaAdi6GrL33ufPLh7ZWkw8UnwVBa9euhc2mnWF4RHrgaXfY/pKGIChZT/VAzAJRcJMkCVqs/ddaUBZIPguCODMzUfs4vzW2PwiqttpxuLQWANA7UT+ZIF6IiQKDAxJ+xa9iRBrhaT1QbomzHqhTtAUdIs1qNsmneCEmCgxZdo4QIwZBRJrh8LB+rnG5DD1NkggwE0QUSPwS4uRRd9jFF1/s1n5msxkJCQk455xzMGPGDKSlpXlyOqKQEHojw3gRJgoUg0FibTQ8DILWr18PwFn01dLor9PvW7p0Kf785z/j73//O+6++26PGkoU7LxdOFVPQZAEwKiBxSSJQhW/hDh5dBXKy8vDXXfdBaPRiFmzZuF///sfsrOzkZ2djRUrVuC6666D0WjEHXfcgU2bNuFvf/sbwsLCcO+99+Kbb75R+zkQBQVPgiC7Q8HB485MUB8dBUEyL8BEAcXRmU4eZYI2b96Ml156CV9++SUuueSSJvcNHjwYl19+Oa6//npcdtllOO+88/Dggw8iMzMT48ePx0svvYRLL71UlcYTBRNPgqBDJ2tgcwhEWgxIiQvzQat8g99CiQKLn0Enj0LBp59+GhdeeOFZAdDpLrnkEowePRrPPPMMAGDcuHEYMmQItm7d6llLiYKcJ0GQqyssMVpXoz1YFE0UWLIsQUeXDJ/xKAjau3evW/MApaamYt++fa6fe/XqhbKyMk9OSRTUPF2wcb9r5Xh9jQxjKp4o8Pg59DAIioiIwLZt21pdEkMIgW3btiEiIsK1ra6uDjExMZ6ckiioebxwqg6XywCYCSLSAqMWp7P2M4+CoAkTJiA3Nxd33HEHampqzrq/trYWd911F3Jzc5vU/xw4cIDD5Ima4clM0UIIHGiYKLFPsr6CIBMvvkQBx7ogDwujFy1ahNWrV+PVV1/F0qVLMWnSJFdwc/jwYXz99dc4deoUOnXqhL/+9a8AnF1oOTk5uP/++9VrPVGQ8KQeqLC8DpV1dhhlCRkdI33QKt+QJQkSixGIAo4ZWQ+DoPT0dGRlZeG2227D2rVrsXTp0rP2GT9+PF599VWkp6cDALp3747CwkLExsZ612KiIORJd9iBhkkSu3eKhElHc+7w2yeRNrAmyIsFVHv06IHVq1fj4MGD+O6771BYWAgASElJwfnnn4+ePXs22d9isSApKcm71hIFKcWDICjHVRStr64wA7vCiDTBIEuQAI8GZQQLr1eR79GjB3r06KFGW4hClieZoMaRYXqaJBFgJohIS4wGGTaHEuhmBIxHubD77rsPP/74o9ptIQpJQgiPCqO5cCoReSvUP48eBUHPPvsshg0bhoEDB2LRokXIz89Xu11EIcOToujyGhuKK6wAgF6JessEsQ6BSCtCPTPr0dXohRdewMiRI/Hzzz/j//7v/9C9e3eMGTMG//znP3Hq1Cm120gU1BxeZIE6x4UjKszrXm2/kcBvnkRaEuqfR4+CoDvuuAObN29Gbm4uHn30UfTs2RObNm3CH/7wB6SkpODKK6/Exx9/DKvVqnZ7iYKOR8tllLArjIi8p6eRpb7g1bPv3r07Hn74YeTk5OCHH37AnXfeifj4ePzvf//DjBkzkJSUhJtuukmtthIFJY+CoCLn8Hi9zRTNrjAibWkcIRaqVLsiDR8+HM899xyOHDmCb775Btdeey0qKirw9ttvq3UKoqDk1cKpOpspmsPjibQnlDO0qn8t27hxIz766CN8/fXXah+aKCi1NwiqszmQf9K5XI3eusNCvQiTSItCOUOrSkVldnY23nvvPXzwwQc4duwYhBCIjo7GDTfcgFmzZqlxCqKg1d4g6Jfj1XAIgQ4RJnSKsvioVb7BIIhIewwGCbAHuhWB4XEQlJeXh/fffx/vv/8+9u3bByEETCYTLr/8csyaNQtXXHEFwsLC1GwrUdBRFNHu2Vr3nzZTtJ7W4JKk0E67E2lVKH858SgIGjVqFLZu3QrRMLT3/PPPx6xZs3DttdciPj5e1QYSBTNvhsfrrSvMYjDoKmgjChUMgtppy5Yt6Nu3L2bNmoVZs2ahW7duLe6rKArkEO5vJGqNZ0XR+hwZZjHxOkCkRUaDHLJriHl0Vdq+fbtrosSWAqCdO3diwYIF6NKlizfta1G3bt0gSdJZt3nz5gEAxo4de9Z9c+fO9UlbiDzV3iDIoQgccM0RpJ8gSAJgMTIIItIqOUSzQR5lgoYOHdrs9sOHD+O9997Du+++i71790II4bP09w8//ACHw+H6ec+ePbjkkktwzTXXuLbdcsstePzxx10/R0RE+KQtRJ5q78KpR07VoM6mwGKU0TVeP+9ns1FmVxiRhhllyaPMtN55PTqssrISH3/8Md59911s3LgRQggIIdC5c2dce+21mDlzphrtPEunTp2a/Pzkk0+iR48euOiii1zbIiIikJyc7JPzE6lBaedFJ6fImQXqmRilqyLjMJMh0E0golYYDTKs9tBbTd6jIMjhcOCrr77CO++8gxUrVqCurs5VJC1JEtavX48LL7zQb9/86uvr8e6772LBggVNztmYlUpOTsaUKVPw0EMPtZoNslqtTZb6qKio8Gm7idqbCTpQ4qwH6sOuMCJSUagWR7crCPrhhx/wzjvv4MMPP8SJEydcw+KnTp2K6667Dk899RS2bduGMWPG+Kq9zVq+fDnKyspw4403urb97ne/Q3p6OlJTU7Fr1y488MADyMnJwaefftricRYtWoTHHnvMDy0mclLaOTqsMRPUS0cjw9gVRqR9esosq0kSou2r8BNPPIH33nsP+/fvbzIs/rrrrsNvf/tb17D4Cy+8EN9//32TWh1/mDhxIsxmM1asWNHiPmvXrsX48eORm5uLHj16NLtPc5mgtLQ0lJeXIyYmRvV2U2izOxScrK53e38hBCa/8C1O1djw5o0jMCA11oetU09MmAnhZnaHEWmZEAIllf5f9LxjlMUnAVhFRQViY2Pb/PvtVibo4YcfhiRJSE5Oxh/+8Ic2h8X7U35+PlavXt1qhgcAMjMzAaDVIMhiscBi0dcMvKRf7Z0j6ERVPU7V2CBLQI9O+skEsSuMSPskSYIhBIuj3b46CSFQVFSEr7/+GqtWrUJZWZkPm+W+t956C4mJibj88stb3S87OxsAkJKS4odWEbWtvRebxkkSJUnCF7sLfdEk1ZkNcsgOvSXSm1CsC3IrCNqyZQvmzZuHhIQEbNq0CXPnzkVKSgqmT5+OTz/9FDabzdftbJaiKHjrrbcwe/ZsGI2/JrUOHjyIv/zlL9i+fTsOHTqE//3vf7jhhhswZswYDB48OCBtJTpTe4OgvYUVrse9/X2+L5qkOk6QSKQfoVgX5NYVauTIkXjppZdw7NgxfPbZZ7j66qshSRKWLVuGa665BikpKbjttttQXFzs6/Y2sXr1ahQUFOCmm25qst1sNmP16tW49NJL0bdvX9x7772YPn16qzVDRP7W3iBoe/4pAEBMmBGzz0/3RZNUZzGyFohIL0JxNXm3CqObU1FRgY8//hjvvPMOvv322yYTI95///2YMWMGzjnnHDXb6nfuFlYReeJkldXtIfJ1NgcmPLsBNofAx7eNQtcE7U+UaDLIiI80B7oZROSmeruCUzXuD9ZQQ6ALoz0O+2JiYjBnzhysX78ehw4dwl//+lf07dsXQggsXrwYw4cPR79+/fCXv/zF01MQBbX2FEbvOVoOm0OgU5QFafHhPmyVelgQTaQvJgO7wzySlpaGhQsX4qeffsK2bdtw5513IjExETk5OXj00UfVOAVRUFEUgfbkYLc1dIUNT++gmzl3OEs0kb5IkgRZJ9cXtaj+VW3YsGF47rnncPToUXz++eeYMWOG2qcg0r32Do9vrAcalh7ng9aozyhLIVlkSaR3oTZCzOu1w1oiyzImT56MyZMn++oURLrVnqLo2noHfj7mHBk2PL2Dr5qkKguzQES6ZDBIgH/nOw4odtoTBUB7gqAfj5TBrggkxVjQOU4f9UBhrAci0iVTiI0QC61nS6QR7Vk4dUeBvuqBDLIEo4GXFiI9CrVubF6piAJAaUcQtP20omg9YEE0kX6FWk0QgyCiAHA3E1RttWPvMedyGXoJgjg0nki/ZFmCDhLOquHViigAFDdHh/14pAwOIZAaF4aUWO3XA8mSBBO7woh0LZTqgkLnmRJpRHuKovXXFcZLCpHeGUJo0kResYj8zK4obu+rtyCIa4UR6V8o1QUxCCLyM3djoKo6O3KK9FMPJEsSzKwHItK9UBohxisWkZ+5mwnaefgUFAGkxYcjMTrMx63ynoVdYURBIZRWkw+dZ0qkEe5mglxLZXTVfhYI4KgwomBhCKERYrxqEfmZu5kgPdUDSRLrgYiCSahkg0LjWRJpiDuLp5bX2nCguAqAPoIgBkBEwSVU6oIYBBH5kaIIuDNFUHZBGQSAbgkR6Bhl8Xm7vMWh8UTBJVRGiPlsFXkiOps7WSAA2JZfCkAfWSBJAsycIJEoqBh9PFeQBMBiMiDQsRaDICI/cneixB35ZQD0EQRZDAZdLOxKRO7zVU2QLEmIMBsQbjJADnQEBAZBRH7lThB0qroeuced9UB6GBnGofFEwccgS5AAuD+/fetMBhkRZgMsRllTX5oYBBH5kTsLp+4ocI4K69EpEh0izb5uklckcGg8UbAyyJLbiz23JMxoQLjZoNmJVBkEEfmR4sYFRU9D480a+1ZHROoxyjLsiqPdj5MkINxkQITZqPlRZgyCiPzInW9VegqCwkwcGk8UrIwGCbC7v79B/rXeRy9fjhgEEfmR0sbosJNVVhw6WQMJwFCN1wOxK4wouLmbxTEbZERYDLqcL4xBEJGfuFMU3ZgF6pUUhdhwk6+b5BV2hREFt9bmCpIAhJkNiDAZYNTxFBkMgoj8xJ3lMvS0Xpgev/URkfuMBvmsEWJaG+LuLQZBRH7izpJhOwrKAOijHohdYUTBT5YlOBThGuIebHWADIKI/KStTNDxSisKSmsgS8DQrnH+aZSHzAY5KL4FElHrIhtGeGl1iLu3GAQR+UlbmaDGrrDeSdGIDtN2PRAnSCQKDeHm4Mr8nIlXMiI/aSsTpKeh8awHIqJgwCCIyE/aWjxVL0GQySBrfgI0IiJ3MAgi8gNFEWgtBioqr8PRsloYJAnnpMX5rV2eYEE0EQULXs2I/MDdLFDflGhEWrRdqhdso0OIKHTpMgh69NFHIUlSk1vfvn1d99fV1WHevHlISEhAVFQUpk+fjuLi4gC2mEJdWxMl6qUrzChL7AojoqChyyAIAAYMGIDCwkLXbdOmTa777rnnHqxYsQIff/wxNmzYgGPHjmHatGkBbC2FutaCICGEboIgC7NARBREtJ13b4XRaERycvJZ28vLy/HGG2/g/fffx8UXXwwAeOutt9CvXz9s3rwZ5513nr+bStRqd9ixsjoUVdTBIEsY0iXOf43yQBjrgYgoiOj2inbgwAGkpqaie/fumDVrFgoKCgAA27dvh81mw4QJE1z79u3bF127dkVWVlarx7RaraioqGhyI1KDw9FyELS9wJkFGpAao+k5OQyypOs1goiIzqTLK1pmZiaWLFmCr776Cq+++iry8vJw4YUXorKyEkVFRTCbzYiLi2vymKSkJBQVFbV63EWLFiE2NtZ1S0tL8+GzoFDSWibI1RWm8fXCWBBNRMFGl91hkydPdv178ODByMzMRHp6Oj766COEh4d7fNyFCxdiwYIFrp8rKioYCJEqlBZqgnRVD8SuMCIKMkFxVYuLi0Pv3r2Rm5uL5ORk1NfXo6ysrMk+xcXFzdYQnc5isSAmJqbJjchbDkWgpTzQ4VO1OF5phckgYVCXWL+2qz1kSYKJXWFEFGSC4qpWVVWFgwcPIiUlBcOHD4fJZMKaNWtc9+fk5KCgoACjRo0KYCspVLW2XEZjFmhgaqymu5vCuFYYEQUhXXaH3XfffZgyZQrS09Nx7NgxPPLIIzAYDJg5cyZiY2MxZ84cLFiwAPHx8YiJicEdd9yBUaNGcWQYBURrS4bppytMuwEaEZGndBkEHTlyBDNnzsTJkyfRqVMnjB49Gps3b0anTp0AAM899xxkWcb06dNhtVoxceJE/OMf/whwqylUtZQJ0ks9kCxJMLMeiIiCkCREG/P5h7CKigrExsaivLyc9UHksfIaG+rsjrO2552oxozXN8NilLF6wUWaDTTCzQbEhJkC3QwiIre5+/dbm1ddoiDSUiaoMQs0qHOsZgMggKPCiCh48epG5GMtzRGkh64wSWI9EBEFLwZBRD6kKALNxUCKENihgyCIARARBTMGQUQ+1FIWKO94NcpqbQgzyeifqt16Mw6NJ6JgxisckQ+1tHp8Y1fY4C5xmp2EUJIAs0bbRkSkBl7hiHyorSBI011hBgMkSQp0M4iIfIZBEJEPNdcdpgiBHYd1EASxK4yIghyvckQ+5HCcHQTlllShotaOCLMB/ZKjA9Cqtkng0HgiCn68yhH5UHOZoMausCFpcTBqtObGbJTZFUZEQU+bV2CiIKE0UxOkh3ogLS/mSkSkFgZBRD7iUATODIEcisDOgjIAwAiNBkHsCiOiUMErHZGPNLdcxv7iSlRZ7YiyGNE7SZv1QOwKI6JQwSCIyEeaWzJsW0NX2NCucTDI2gw0OEs0EYUKBkFEPtJcJkgfS2XwskBEoYFXOyIfOTMGsisKsg+XAdBuEGQ2yJA1mqEiIlIbgyAiHzkzE7SvsBI19Q7EhBnRMzEqQK1qHSdIJKJQwisekY+cOUfQdlc9UAfIGiw8liQgnEPjiSiEMAgi8gEhBM6cJ1Hr8wNFmI0cFUZEIYVBEJEP2M+YJNHmUPDjkTIA2gyCJAmIYBaIiEIMgyAiHzhz9fifj1WgzqYgLtyE7p0iA9SqlkWYjSyIJqKQwyCIyAfODIIau8KGpWuvHkgCs0BEFJoYBBH5QEtF0VrsCgs3G5gFIqKQxCCIyAccjl+DoHq7gt1HywFoLwiSAESajYFuBhFRQDAIIvKB0zNBe46Ww2pXkBBpRreEiAC26mzMAhFRKGMQROQDymk1QTsKfq0H0tIQdGaBiCjUMQgiUplDETi9Ikir9UBhzAIRUYhjEESkstOXy6izOTRZD8QsEBERgyAi1Z2+ZNieo+WwOQQ6RVmQ1iE8cI06Q5jZAAOzQEQU4hgEEans9EzQttO6wrRSD8QsEBGRE4MgIpWdngnSYj2QxcQsEBERwCCISHWNw+Nr6x34+VgFAG0FQVEWZoGIiAAGQUSqa+wO+/FIGeyKQHJMGFLjwgLcKqcwZoGIiFwYBBGpSAiBxnkSt2uwHijSzDXCiIga6TIIWrRoEUaOHIno6GgkJibiyiuvRE5OTpN9xo4dC0mSmtzmzp0boBZTqKiy2l3/1lo9UJjRAKNBlx95IiKf0OUVccOGDZg3bx42b96MVatWwWaz4dJLL0V1dXWT/W655RYUFha6bk899VSAWkzBTlEETlXXo6beAQCottqxr7ASgHaCoEgLs0BERKfTZYXkV1991eTnJUuWIDExEdu3b8eYMWNc2yMiIpCcnOzv5lGIsTkUlNXYoJy2XtiPR8rgEAKd48KRHBv4eiBmgYiIzhYUV8XycueMvPHx8U22v/fee+jYsSMGDhyIhQsXoqamptXjWK1WVFRUNLkRtaam3o5T1fVNAiBAe11hEcwCERGdRZeZoNMpioK7774bF1xwAQYOHOja/rvf/Q7p6elITU3Frl278MADDyAnJweffvppi8datGgRHnvsMX80m3ROCIGKOjvqbI5m79dSEGQxyjAxC0REdBZJiDO+wurM7bffji+//BKbNm1Cly5dWtxv7dq1GD9+PHJzc9GjR49m97FarbBara6fKyoqkJaWhvLycsTExKjedtInhyJQVlMPu9L8R6eqzo5LntsARQAr7rgAidGB7Q6LjzQzCCKikFJRUYHY2Ng2/37rOhM0f/58rFy5Ehs3bmw1AAKAzMxMAGg1CLJYLLBYLKq3k4KH1e5Aea0NrX112Hn4FBQBpMWHBzwAYhaIiKhlugyChBC44447sGzZMqxfvx4ZGRltPiY7OxsAkJKS4uPWUbCqstpRfdoQ+Ja4usK6Br4rLIJrhBERtUiXV8h58+bh/fffx2effYbo6GgUFRUBAGJjYxEeHo6DBw/i/fffx2WXXYaEhATs2rUL99xzD8aMGYPBgwcHuPWkN4oiUFFng9WutL0ztFMPZDbIMBuZBSIiaokug6BXX30VgHNCxNO99dZbuPHGG2E2m7F69Wo8//zzqK6uRlpaGqZPn44///nPAWgt6Vlzw99bU15rw4HiKgCBD4IiuUYYEVGrdHmVbKuWOy0tDRs2bPBTayhY1dY7UFlnQ3tGDuwsOAUBoFtCBBKiAldfxiwQEVHbdBkEEfmSEAKVVjtq65sf/t4arXSFcV4gIqK2MQgiOo1DESivtcHmcK/+50w78ssABDYIMhlkWIwMgoiI2sIgiKiBO8PfW+JQBL7YXYjc4856oMLyOpVb5z6uEUZE5B4GQURwLnha5cbw9+ZsyTuJF9fkIrekyrXt421HcN156Wo1z23MAhERuY9BEIU0IZzdX+4Ofz9dbkkVXlp7AJt/KQUARIcZMTy9A/YWVmD2+f4PgAAgwswAiIjIXQyCKGTZHQrKam1wtLD8RUuOV1rxz40H8fmuQigCMMoSrh7eBTddkIHYCJOPWts2oywhzMQgiIjIXQyCKCTV2RyoqG3f8Peaejve3VyA97bko87mzByN75uIP4zrgS4dInzT0HbgvEBERO3DqyaFFE+Gv9sVBSt+LMTrG39BaXU9AGBwl1jceXEvDOoS66umtguzQERE7ccgiEKCEAI19Q5U19vdHv0lhMB3B0/i5bW5yDtRDQDo0iEc88b1xLg+nSBJkg9b3D7MAhERtR+vnBTUFEWgxuZATTuCHwDYV1SBl9bkYlvD5Icx4UbcPLo7pg3rrLlV2Q3MAhEReYRBEAUlT4Of4oo6vLr+IL7c41yU12yQce3INMw+Px3RYYErem5NFLNAREQe4dWTgoqiCFTXO2t+2lP0XFVnx9tZh/DhD4ddw+UnDUjG3LHdkRIb7pvGqoBZICIizzEIoqDgaAh+6toZ/NgdCpbtPIp/f5uHslobAGBY1zjcOb4X+qXE+KaxKoo08yNMROQpXkFJ1zwNfoQQ2Lj/BF5el4uC0hoAQHp8BO4Y3xOje3bUVNFzS2RJQjgnRyQi8hiDoAApq6mHIgCDJEGWnd0aBlmCQXL+Vw9/hAPJoQhUWe2w2toX/ADAT8fK8eKaXGQfLgMAdIgw4ZYLu+OKc1Jh1FjRc2tYC0RE5B1eRQPEoQjYFQFbC/dLElwBUeNNln4NlGQ5NIMku0NBdb0Ddba25/kRQqCk0oq8E9XIO1GNQyeq8cOhUhwtcy5uajHK+N25XXHdqHTdBRSyJCHMpJ+AjYhIi/R15Q8hQgB24QyUmnN6kCSflkEK1iDJ7lBQbXWgzn528KMIgcKyOmewc/LXgCfvRDVqWpgUMdwk48PbRiEpJszXTfeJSIuB2UIiIi8xCAqAbYdKcbzSCoMswWyUYTbIMBllWIwyTAb5122GlrvF2gySAFdwJDcJjqCrLjebQ0FNQ/Bjdyg4cqoWhxoCHWewU4NDJ6tbXADVIEtI6xCObh0jkdExEicqrdj8y0ncNDpDtwGQLEkI54gwIiKvMQgKgLnvbseJqnq39nUGSBLMDcGRydBMsGSUYWn4r9kg42hZLfYXV2JAagwGpMbCZJBgMsgNNwlGg+w6rsUgw2I0wGJq+t9wk/P4FqPBda7G45gNsk8zTVa7AweKq/DTsXLsL65yZXYKSmtaDPrMBhldEyKQ0TES3Rr+m9ExEmnxEZqb3NBbzAIREamDQVAAVNXZAQAGCUiIssDmUGC1K7A5FNgcTf/I1zsU1DuAari/1lWjHw6dwg+HTqnSZk+d+be6uT/d0mlbFSFaLXQONxnQrWMEuiVEugKdjI6RSI0Lh6GNwEySmp7L15o8k+b/2W6SBGaBiIhUwiAoAP78m/74x7pczLmwO64dmdbkPkURsDkUZ/BjV2C1OVDfGCTZFde/6xtvjl//bW349478U9hRcAoDO8eie6co2BuCq3pXoOV8nN0hfv3ZLmBTTvu3Q/n1cQ4FjhYyMG05c7bm5o9y9lYJwKAusejeMRLdO0Whe8dI9OgUheS4MFeRuIRfA5vGYEuWpIZtgHT6PhrMnIjTfjmN/xQt3d/w38bnRURE3pOEaM+iAqGloqICsbGxKC8vR0yM9ifO8yWH8mvA9P6WArz1/SHceH43XD28i2uflt5JLeZ2mtn8yY4j+E9WPuaN64HrzuvmfcOJiCjkuPv3m0FQKxgEERER6Y+7f7+Dq2KUiIiIyE0MgoiIiCgkMQgiIiKikMQgiIiIiEISgyAiIiIKSQyCiIiIKCQxCCIiIqKQxCCIiIiIQhKDICIiIgpJDIKIiIgoJAV9EPTKK6+gW7duCAsLQ2ZmJrZu3RroJhEREZEGBHUQ9OGHH2LBggV45JFHsGPHDgwZMgQTJ05ESUlJoJtGREREARbUC6hmZmZi5MiRePnllwEAiqIgLS0Nd9xxBx588MGz9rdarbBara6fKyoqkJaWxgVUiYiIdCTkF1Ctr6/H9u3bMWHCBNc2WZYxYcIEZGVlNfuYRYsWITY21nVLS0vzV3OJiIjIz4yBboCvnDhxAg6HA0lJSU22JyUlYd++fc0+ZuHChViwYIHr5/LycnTt2hUVFRU+bSsRERGpp/HvdludXUEbBHnCYrHAYrG4fm78JTIjREREpD+VlZWIjY1t8f6gDYI6duwIg8GA4uLiJtuLi4uRnJzs1jFSU1Nx+PBhREdHQ5Ik1drWWGt0+PDhkKg1CqXny+cavELp+fK5Bq9Qeb5CCFRWViI1NbXV/YI2CDKbzRg+fDjWrFmDK6+8EoCzMHrNmjWYP3++W8eQZRldunTxWRtjYmKC+k14plB6vnyuwSuUni+fa/AKhefbWgaoUdAGQQCwYMECzJ49GyNGjMC5556L559/HtXV1fj9738f6KYRERFRgAV1EHTttdfi+PHjePjhh1FUVIRzzjkHX3311VnF0kRERBR6gjoIAoD58+e73f3lLxaLBY888kiTIuxgFkrPl881eIXS8+VzDV6h9nzbEtSTJRIRERG1JGgnSyQiIiJqDYMgIiIiCkkMgoiIiCgkMQgiIiKikMQgKABeeeUVdOvWDWFhYcjMzMTWrVsD3SSvLVq0CCNHjkR0dDQSExNx5ZVXIicnp8k+Y8eOhSRJTW5z584NUIs99+ijj571PPr27eu6v66uDvPmzUNCQgKioqIwffr0s2Yu15Nu3bqd9XwlScK8efMA6Pt13bhxI6ZMmYLU1FRIkoTly5c3uV8IgYcffhgpKSkIDw/HhAkTcODAgSb7lJaWYtasWYiJiUFcXBzmzJmDqqoqPz4L97T2XG02Gx544AEMGjQIkZGRSE1NxQ033IBjx441OUZz74Unn3zSz8/EPW29tjfeeONZz2XSpElN9gmG1xZAs59fSZKwePFi1z56em3VxCDIzz788EMsWLAAjzzyCHbs2IEhQ4Zg4sSJKCkpCXTTvLJhwwbMmzcPmzdvxqpVq2Cz2XDppZeiurq6yX633HILCgsLXbennnoqQC32zoABA5o8j02bNrnuu+eee7BixQp8/PHH2LBhA44dO4Zp06YFsLXe+eGHH5o811WrVgEArrnmGtc+en1dq6urMWTIELzyyivN3v/UU0/hxRdfxGuvvYYtW7YgMjISEydORF1dnWufWbNm4aeffsKqVauwcuVKbNy4Ebfeequ/noLbWnuuNTU12LFjBx566CHs2LEDn376KXJycjB16tSz9n388cebvNZ33HGHP5rfbm29tgAwadKkJs9l6dKlTe4PhtcWQJPnWFhYiDfffBOSJGH69OlN9tPLa6sqQX517rnninnz5rl+djgcIjU1VSxatCiArVJfSUmJACA2bNjg2nbRRReJu+66K3CNUskjjzwihgwZ0ux9ZWVlwmQyiY8//ti1be/evQKAyMrK8lMLfeuuu+4SPXr0EIqiCCGC53UFIJYtW+b6WVEUkZycLBYvXuzaVlZWJiwWi1i6dKkQQoiff/5ZABA//PCDa58vv/xSSJIkjh496re2t9eZz7U5W7duFQBEfn6+a1t6erp47rnnfNs4H2ju+c6ePVtcccUVLT4mmF/bK664Qlx88cVNtun1tfUWM0F+VF9fj+3bt2PChAmubbIsY8KECcjKygpgy9RXXl4OAIiPj2+y/b333kPHjh0xcOBALFy4EDU1NYFontcOHDiA1NRUdO/eHbNmzUJBQQEAYPv27bDZbE1e4759+6Jr165B8RrX19fj3XffxU033dRkUeFgeV1Pl5eXh6KioiavZWxsLDIzM12vZVZWFuLi4jBixAjXPhMmTIAsy9iyZYvf26ym8vJySJKEuLi4JtuffPJJJCQkYOjQoVi8eDHsdntgGqiC9evXIzExEX369MHtt9+OkydPuu4L1te2uLgYn3/+OebMmXPWfcH02ror6GeM1pITJ07A4XCctWxHUlIS9u3bF6BWqU9RFNx999244IILMHDgQNf23/3ud0hPT0dqaip27dqFBx54ADk5Ofj0008D2Nr2y8zMxJIlS9CnTx8UFhbisccew4UXXog9e/agqKgIZrP5rD8cSUlJKCoqCkyDVbR8+XKUlZXhxhtvdG0Lltf1TI2vV3Of18b7ioqKkJiY2OR+o9GI+Ph4Xb/edXV1eOCBBzBz5swmi2zeeeedGDZsGOLj4/H9999j4cKFKCwsxLPPPhvA1npm0qRJmDZtGjIyMnDw4EH86U9/wuTJk5GVlQWDwRC0r+3bb7+N6Ojos7rog+m1bQ8GQaS6efPmYc+ePU3qZAA06UsfNGgQUlJSMH78eBw8eBA9evTwdzM9NnnyZNe/Bw8ejMzMTKSnp+Ojjz5CeHh4AFvme2+88QYmT56M1NRU17ZgeV3JyWaz4be//S2EEHj11Veb3LdgwQLXvwcPHgyz2YzbbrsNixYt0t0yDDNmzHD9e9CgQRg8eDB69OiB9evXY/z48QFsmW+9+eabmDVrFsLCwppsD6bXtj3YHeZHHTt2hMFgOGukUHFxMZKTkwPUKnXNnz8fK1euxLp169ClS5dW983MzAQA5Obm+qNpPhMXF4fevXsjNzcXycnJqK+vR1lZWZN9guE1zs/Px+rVq3HzzTe3ul+wvK6Nr1drn9fk5OSzBjXY7XaUlpbq8vVuDIDy8/OxatWqJlmg5mRmZsJut+PQoUP+aaAPde/eHR07dnS9b4PttQWAb7/9Fjk5OW1+hoHgem1bwyDIj8xmM4YPH441a9a4timKgjVr1mDUqFEBbJn3hBCYP38+li1bhrVr1yIjI6PNx2RnZwMAUlJSfNw636qqqsLBgweRkpKC4cOHw2QyNXmNc3JyUFBQoPvX+K233kJiYiIuv/zyVvcLltc1IyMDycnJTV7LiooKbNmyxfVajho1CmVlZdi+fbtrn7Vr10JRFFcwqBeNAdCBAwewevVqJCQktPmY7OxsyLJ8VreRHh05cgQnT550vW+D6bVt9MYbb2D48OEYMmRIm/sG02vbqkBXZoeaDz74QFgsFrFkyRLx888/i1tvvVXExcWJoqKiQDfNK7fffruIjY0V69evF4WFha5bTU2NEEKI3Nxc8fjjj4tt27aJvLw88dlnn4nu3buLMWPGBLjl7XfvvfeK9evXi7y8PPHdd9+JCRMmiI4dO4qSkhIhhBBz584VXbt2FWvXrhXbtm0To0aNEqNGjQpwq73jcDhE165dxQMPPNBku95f18rKSrFz506xc+dOAUA8++yzYufOna4RUU8++aSIi4sTn332mdi1a5e44oorREZGhqitrXUdY9KkSWLo0KFiy5YtYtOmTaJXr15i5syZgXpKLWrtudbX14upU6eKLl26iOzs7CafYavVKoQQ4vvvvxfPPfecyM7OFgcPHhTvvvuu6NSpk7jhhhsC/Mya19rzraysFPfdd5/IysoSeXl5YvXq1WLYsGGiV69eoq6uznWMYHhtG5WXl4uIiAjx6quvnvV4vb22amIQFAAvvfSS6Nq1qzCbzeLcc88VmzdvDnSTvAag2dtbb70lhBCioKBAjBkzRsTHxwuLxSJ69uwp7r//flFeXh7Yhnvg2muvFSkpKcJsNovOnTuLa6+9VuTm5rrur62tFX/4wx9Ehw4dREREhLjqqqtEYWFhAFvsva+//loAEDk5OU226/11XbduXbPv29mzZwshnMPkH3roIZGUlCQsFosYP378Wb+DkydPipkzZ4qoqCgRExMjfv/734vKysoAPJvWtfZc8/LyWvwMr1u3TgghxPbt20VmZqaIjY0VYWFhol+/fuJvf/tbk6BBS1p7vjU1NeLSSy8VnTp1EiaTSaSnp4tbbrnlrC+jwfDaNvrnP/8pwsPDRVlZ2VmP19trqyZJCCF8mmoiIiIi0iDWBBEREVFIYhBEREREIYlBEBEREYUkBkFEREQUkhgEERERUUhiEEREREQhiUEQERERhSQGQURERBSSGAQRkU9JktTm7cYbb/T6PDfeeCMkScL69eu9PpZatNgmIvqVMdANIKLQMHv27BbvGz16tB9bQkTkxCCIiPxiyZIlPj3+okWL8OCDD6Jr164+PQ8RBQ8GQUQUFFJSUpCSkhLoZhCRjrAmiIg0R5IkdOvWDfX19XjkkUfQo0cPhIWFoXv37nj44YdRV1d31mNaqr85fvw4HnzwQfTv3x9RUVGIjY1F7969ccMNN2Dr1q1nHefnn3/GrFmzkJKSArPZjM6dO+OGG25ATk5Oi+198803cc455yA8PBzJycm48cYbUVRU1OpzLC0txcKFC9G/f3+Eh4cjNjYWF198MVauXOneL4mIvMYgiIg0SQiB6dOnY/Hixejfvz8uv/xylJaW4i9/+Qt+85vfwOFwtHmMyspKZGZm4u9//zuqqqpwySWX4NJLL0WHDh3wwQcf4Isvvmiy/5o1azBixAi8//77SElJwfTp05GYmIh33nkHI0aMwLfffnvWOR588EHMmTMHP//8M8aMGYMxY8bgyy+/RGZmJkpLS5tt1/79+3HOOefgySefRG1tLSZOnIgRI0Zgy5YtmDJlCp5++mnPfmlE1D6CiMiHAIj2XmoaH9OlSxdx8OBB1/aSkhIxcOBAAUA899xzTR4ze/ZsAUCsW7fOte3NN98UAMTUqVOFw+Fosn9JSYnYvXu36+eqqiqRlJQkAIiXX365yb7PPvusqz21tbWu7VlZWUKSJBEbGyt27Njh2l5ZWSkuvvhi1/M4vU12u10MGjRIABBPPfVUk3YdOHBAZGRkCIPB0KRtROQbzAQRkV+0NkR++fLlzT7m4YcfRvfu3V0/d+rUCYsXLwYAvPzyy22e8/jx4wCAiy++GLLc9HLXqVMnDBw40PXzRx99hOLiYowaNQrz5s1rsu8999yD4cOH48iRI/jkk09c21999VUIIXDXXXdh6NChru1RUVF46aWXIEnSWW1asWIFdu/ejenTp+P+++9v0q6ePXvimWeegcPhwL/+9a82nx8ReYeF0UTkF60NkW9pRNeMGTPO2jZp0iR06NABBw8eRGFhYavF0MOHDwcALF68GElJSbj88ssRHR3d7L6NXV2zZs1q9v7rrrsO27dvx7fffuvap/ExzbWzf//+GDJkCLKzs5ts/+abbwAA06ZNa/Y8F154IQA0W69EROpiEEREftHeIfIdOnRoMWBJT0/HqVOncOzYsVaDoPHjx+Oee+7B888/j5kzZ8JoNGLYsGG45JJLcNNNNzXJMh07dgwA0K1bt2aP1bj96NGjZz0mPT29xcecGQQdOnQIgDPYaingAoATJ060eB8RqYNBEBEFtWeffRa33XYbPvvsM6xevRrfffcdtm7diqeeegpLly7F9OnT3TpOc11bnlAUBYAzo5WUlNTifh07dlTlfETUMgZBRKRJp06dQmVlZbPZoIKCAgBAamqqW8fq06cP/vjHP+KPf/wj6urq8PLLL+P+++/H7bff7gqCGo+Vn5/f7DEaMzidO3d2bUtJScGhQ4eQn5+Pfv36nfWY5o7VpUsXAMDNN9/sdgBGRL7Bwmgi0qyPPvrorG3ffPMNSktL0b17d48mRwwLC8N9992HlJQUHD9+HCUlJQB+rcVZunRps4979913m+x3+r+ba+e+ffvO6goDgEsuuQQAsGzZsna3nYjUxSCIiDTrsccec2VgAGedzP333w8AZ43gas7y5cuxefPms7Zv374dxcXFiIqKQlxcHADgt7/9LZKSkrBp0ya8/vrrTfZ/8cUXsW3bNnTu3LlJ9mbu3LkAgOeffx4//vija3t1dTXuuOMOCCHOOvf06dPRv39/vPfee/jLX/4Cq9Xa5H4hBL777jt89913bT4/IvIOu8OIyC9aWym+a9euePzxx8/aNnjwYAwYMADjx4+HyWTC2rVrUVZWhnHjxuHOO+9s85zr16/HCy+8gM6dO2Po0KGIiYnBsWPH8O2330JRFDz22GMwm80AgMjISLz33nuYMmUKbrvtNrz++uvo3bs39u3bh507dyIqKgpLly5FWFiY6/jnn38+7rvvPjz99NMYOXIkLr74YsTGxmLDhg2wWCyYMmUKVqxY0aRNRqMRy5cvx8SJE/Hwww/j5ZdfxuDBg5GYmIgTJ04gOzsbJSUleO6553DBBRe04zdMRO0W4HmKiCjIoWHCwNZuQ4YMOesx6enpoq6uTvzpT38S3bp1E2azWaSnp4v/+7//EzU1NWedp7nJEnfu3CnuvfdeMXLkSJGYmCgsFotIT08XU6ZMEatXr262vXv27BEzZ84USUlJwmQyiZSUFHHdddeJffv2tfgc//Wvf4nBgwcLi8UiEhMTxXXXXSeOHj3abJsalZWViSeeeEIMGzZMREVFibCwMNGtWzcxceJE8corr4jjx4+79fslIs9JQjSTryUiCiBJkpCent6kK4yISG2sCSIiIqKQxCCIiIiIQhKDICIiIgpJHB1GRJrDUkUi8gdmgoiIiCgkMQgiIiKikMQgiIiIiEISgyAiIiIKSQyCiIiIKCQxCCIiIqKQxCCIiIiIQhKDICIiIgpJ/w959Ll/ztPX+QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment and training parameters\n",
    "env_name = 'CartPole-v0'\n",
    "num_episodes_train = 200\n",
    "num_episodes_test = 20\n",
    "learning_rate = 5e-4\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(env_name)\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = 4\n",
    "\n",
    "# Plot average performance of 5 trials\n",
    "num_seeds = 5\n",
    "l = num_episodes_train // 10\n",
    "res = np.zeros((num_seeds, l))\n",
    "gamma = 0.99\n",
    "\n",
    "# Loop over multiple seeds\n",
    "for i in tqdm.tqdm(range(num_seeds)):\n",
    "    reward_means = []\n",
    "\n",
    "    # Create an instance of the DQN_Agent class\n",
    "    agent = DQN_Agent(env_name, lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for m in range(num_episodes_train):\n",
    "        agent.train()\n",
    "\n",
    "        # Evaluate the agent every 10 episodes during training\n",
    "        if m % 10 == 0:\n",
    "            print(\"Episode: {}\".format(m))\n",
    "\n",
    "            # Evaluate the agent's performance over 20 test episodes\n",
    "            G = np.zeros(num_episodes_test)\n",
    "            for k in range(num_episodes_test):\n",
    "                g = agent.test()\n",
    "                G[k] = g\n",
    "\n",
    "            reward_mean = G.mean()\n",
    "            reward_sd = G.std()\n",
    "            print(f\"The test reward for episode {m} is {reward_mean} with a standard deviation of {reward_sd}.\")\n",
    "            reward_means.append(reward_mean)\n",
    "\n",
    "    res[i] = np.array(reward_means)\n",
    "\n",
    "# Plotting the average performance\n",
    "ks = np.arange(l) * 10\n",
    "avs = np.mean(res, axis=0)\n",
    "maxs = np.max(res, axis=0)\n",
    "mins = np.min(res, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, avs, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=15)\n",
    "plt.ylabel('Avg. Return', fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8031648e",
   "metadata": {},
   "source": [
    "The plot above illustrates the average return, representing the total reward received by the agent in an episode, across 5 different runs. It is evident that the agent progressively learns a more effective strategy to achieve higher returns with an increasing number of episodes.\n",
    "\n",
    "Now, let's observe the behavior of the trained agent in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897affea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v0', render_mode='human')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Run the environment for 100 steps\n",
    "for i in range(200):\n",
    "    # Display the current state of the environment\n",
    "    env.render()\n",
    "    # display.display(plt.gcf())\n",
    "    # display.clear_output(wait=True)\n",
    "    \n",
    "    # Choose an action based on the learned Q-network\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = agent.policy_net.net(state)\n",
    "    action = agent.greedy_policy(q_values).detach().numpy()\n",
    "    \n",
    "    # Take the chosen action and observe the next state, reward, and termination status\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    # If the episode is terminated or truncated, reset the environment\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "\n",
    "# Close the environment after exploration\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b093e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v0', render_mode='rgb_array')\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Run the environment for 100 steps\n",
    "for i in range(100):\n",
    "    # Display the current state of the environment\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # Choose an action based on the learned Q-network\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        q_values = agent.policy_net.net(state)\n",
    "    action = agent.greedy_policy(q_values).detach().numpy()\n",
    "    \n",
    "    # Take the chosen action and observe the next state, reward, and termination status\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    # If the episode is terminated or truncated, reset the environment\n",
    "    if terminated or truncated:\n",
    "        state, info = env.reset()\n",
    "\n",
    "# Close the environment after exploration\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870ba75",
   "metadata": {},
   "source": [
    "It properly choose the right or left action to maintain the pole position upright!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299e721",
   "metadata": {},
   "source": [
    "### Ref\n",
    "- Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" nature 518.7540 (2015): 529-533.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
